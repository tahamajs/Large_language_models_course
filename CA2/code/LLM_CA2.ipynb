{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLJYAhNjjFUF"
      },
      "source": [
        "# **CA 2, LLMs Spring 2025**\n",
        "\n",
        "- **Name:**\n",
        "- **Student ID:**\n",
        "\n",
        "---\n",
        "#### Your submission should be named using the following format: `CA2_LASTNAME_STUDENTID.ipynb`.\n",
        "\n",
        "---\n",
        "\n",
        "##### *How to do this problem set:*\n",
        "\n",
        "- Some questions require writing Python code and computing results, and the rest of them have written answers. For coding problems, you will have to fill out all code blocks that say `YOUR CODE HERE`.\n",
        "\n",
        "- For text-based answers, you should replace the text that says ```Your Answer Here``` with your actual answer.\n",
        "\n",
        "- There is no penalty for using AI assistance on this homework as long as you fully disclose it in the final cell of this notebook (this includes storing any prompts that you feed to large language models). That said, anyone caught using AI assistance without proper disclosure will receive a zero on the assignment (we have several automatic tools to detect such cases). We're literally allowing you to use it with no limitations, so there is no reason to lie!\n",
        "\n",
        "---\n",
        "\n",
        "##### *Academic honesty*\n",
        "\n",
        "- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your notebook. If you turn in correct answers on your notebook without code that actually generates those answers, we will consider this a serious case of cheating.\n",
        "\n",
        "- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n",
        "\n",
        "---\n",
        "\n",
        "If you have any further questions or concerns, contact the TAs via email: m.salmani78@ut.ac.ir / mehrabi.m@ut.ac.ir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAV1HzrhLd7c"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4pt9EQLrxBO",
        "outputId": "7f3a40c9-ef2c-4b03-b0b1-3d1cd6aacba4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: Levenshtein in /usr/local/lib/python3.11/dist-packages (0.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from Levenshtein) (3.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tyro\n",
        "!pip install \"protobuf<4.0.0\"\n",
        "\n",
        "\n",
        "\n",
        "!pip install  bitsandbytes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA6CUhoesHWA",
        "outputId": "c813fb47-fec1-41a8-b017-79bf9f8e3e7f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tyro in /usr/local/lib/python3.11/dist-packages (0.9.17)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro) (1.7.1)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro) (4.4.2)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from tyro) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro) (0.1.2)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.11/dist-packages (3.20.3)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.3)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sfDbur2dfcjy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from Levenshtein import ratio\n",
        "from collections import defaultdict\n",
        "from datasets import load_dataset\n",
        "from trl import ORPOConfig, ORPOTrainer\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iNY4jz4Lkwtb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ff8af61-c714-46a9-e32a-e942bd99700f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `LLM_CA2` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `LLM_CA2`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login --token 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"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CSKRoHpW7Qd-"
      },
      "outputs": [],
      "source": [
        "class CONFIG:\n",
        "    seed = 42\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
        "    reward_model_name = \"nicolinho/QRM-Llama3.1-8B-v2\"\n",
        "    benchmark_name = \"openai/gsm8k\"\n",
        "    dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\n",
        "\n",
        "    train_data_size = 1600\n",
        "    benchmark_subset_size = 50\n",
        "    max_seq_length = 2048\n",
        "    train_batch_size = 2\n",
        "    gradient_accumulation_steps = 4\n",
        "    epochs = 1\n",
        "\n",
        "    # LoRA Configs\n",
        "    lora_rank = 64,\n",
        "    lora_alpha = 64,\n",
        "    use_gradient_checkpointing = \"unsloth\"\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "\n",
        "    dpo_output_dir = \"llama-3.2-3b-dpo-checkpoint\"\n",
        "    orpo_output_dir = \"llama-3.2-3b-orpo-checkpoint\"\n",
        "\n",
        "device = CONFIG.device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRuSzJeqJYsJ"
      },
      "source": [
        "### Introductions to unsloth\n",
        "\n",
        "Modern large language models (LLMs) require significant computational resources for fine-tuning and inference. The `unsloth` library is designed to optimize these processes by making training up to 30√ó faster and reducing memory usage by 60%, enabling more efficient model adaptation on consumer-grade GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgSHp25MMnhL"
      },
      "source": [
        "---\n",
        "\n",
        "**Learn More:**\n",
        "\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Eb4IM4iLbW9"
      },
      "source": [
        "### Install and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "j_roQDImZJE5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "!pip install datasets\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "    !pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "neZPEJBplk3w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3625a033-2aa6-465b-e17b-db97b7d5e9a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-e98318b663f4>:1: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  import unsloth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.5.1+cu121 with CUDA 1201 (you have 2.6.0+cu124)\n",
            "    Python  3.11.11 (you have 3.11.11)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "2025.3.18\n"
          ]
        }
      ],
      "source": [
        "import unsloth\n",
        "print(unsloth.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiWagCG4pjww"
      },
      "source": [
        "# In-context Learning (30 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fjyU22wCiOd"
      },
      "source": [
        "### Question 1 (5 points):\n",
        "\n",
        "**a)** What is In-Context Learning (ICL), and how does it differ from fine-tuning? What are its limitations compared to fine-tuning?\n",
        "\n",
        "**b)** Explain what [Chain-of-Thought (CoT)](https://arxiv.org/abs/2201.11903) prompting is and how it works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laSJdBpgqE-M"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npgTNJf7lEAq"
      },
      "source": [
        "### Load Model & Tokenizer (2.5 points)\n",
        "\n",
        "- Load `Llama-3.2-3B-Instruct-bnb-4bit` model using `unsloth` for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Z2JUGaj4lB3w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c102dd25-d670-45de-fe48-54fd3f6500e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-980f3e5f948d>:1: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  from unsloth import FastLanguageModel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.5.1+cu121 with CUDA 1201 (you have 2.6.0+cu124)\n",
            "    Python  3.11.11 (you have 3.11.11)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "def load_model_and_tokenizer(model_id, max_seq_length):\n",
        "    \"\"\"\n",
        "    Loads a 4-bit quantized model and its tokenizer using Unsloth, setting the tokenizer's max length.\n",
        "\n",
        "    Args:\n",
        "        model_id (str): The identifier of the model to load.\n",
        "        max_seq_length (int): The maximum sequence length for the tokenizer.\n",
        "\n",
        "    Returns:\n",
        "        model: The loaded language model.\n",
        "        tokenizer: The tokenizer associated with the model.\n",
        "    \"\"\"\n",
        "    print(f\"Loading model and tokenizer using Unsloth for model: {model_id}\")\n",
        "\n",
        "    # Load the model and tokenizer using Unsloth's FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=model_id,\n",
        "        max_seq_length=max_seq_length,\n",
        "        load_in_4bit=True  # Enables 4-bit quantization for efficient memory usage\n",
        "    )\n",
        "\n",
        "    return model, tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bDLhdusm0m3"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = load_model_and_tokenizer(CONFIG.model_name, CONFIG.max_seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_1wFWx7rWUo"
      },
      "source": [
        "### Load benchmark (2.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftAg-qauF8HW"
      },
      "source": [
        "1. Load the `GSM8K` benchmark dataset.\n",
        "2. Randomly select a subset of `50` samples from the dataset.\n",
        "3. Display one sample from the selected subset.\n",
        "<a id=\"gsm8k_benchmark\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Jfzk-b4Dm8Hi"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def load_gsm8k_dataset():\n",
        "    \"\"\"Load the GSM8K dataset from HuggingFace.\"\"\"\n",
        "    dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def create_sample_dataset(dataset, num_samples, seed):\n",
        "    \"\"\"Create a fixed sample dataset for evaluation.\"\"\"\n",
        "    sample_test = dataset.shuffle(seed=seed).select(range(num_samples))\n",
        "    return sample_test\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kL3-SUvTkpom",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "7c453e750ee44c90a9b9dc8be5d5fb43",
            "c4e2819a336849018ed46dc79734f385",
            "536fa11665f6428b97f620c559961367",
            "36ee5d6a1d7f4c97a36938a2ba223186",
            "45558c900a4e464b8d87f3a65bb92246",
            "3ca4cbcfdae7427dae77a04995d09cf9",
            "a9d1a63481a84d0e8a6a3cbb0b90b96b",
            "b6eada20d71d42c5a190c2732736ea89",
            "190f929bac144b3c8f9ce57f8b2aeaf9",
            "d6e377f98c7f4524a1e7aaffc15f35c9",
            "1f12479477474537bb4c241fce06d6e0",
            "e973bc57470b41bda53e6697d2632669",
            "44aa7d9c8ea445139ab469695f80168a",
            "ce250c8ba31f4bfda00f0d3ded9ecbf0",
            "6b472cb679704c378cdf41ce26b99393",
            "966ee1eab5174e4e960c58f94fbce30b",
            "f46bbd3fe61340fc87a2716c3a18eaaf",
            "459e480df3e342049d0c52f878bdb7c9",
            "f4f59467ce624125b10acda2552db860",
            "8f918727de5f4391a734dbe3f52ec1cc",
            "b72b7986229941ca82876c6bdf31f3ae",
            "4bb8b3924d474acfb40c8b25add33cbb",
            "001290d3ed254e16b8e75eb3ecb2b901",
            "40230e550b2c481bbb70f505f723b6bd",
            "96ddc0f32e3c4c929ccf489a2a1a11b4",
            "3382c376cd064efe9e249eec782b944c",
            "0aa0f249959b4b4ea9f01bd5a6c51d17",
            "1d99a3b72b1746bf9b77e53ededcf34e",
            "8c331e31992c40c79994b7db647e3b8a",
            "71b64e6e0f3c4f3b8fcdeb2615654ee0",
            "af001559668042f3a82d2965fb5cd3d2",
            "5d8aa9820d7b4c44960f059d7f261b93",
            "0add3a61bcd54ad29c883a12f836703d",
            "f4b258bcb78844a085327985e96b44dc",
            "c96680953e814b2d8a63818c3068a32c",
            "d942c03267dd45cfabb6a17f0ac7180c",
            "7e610bc9e1684287926b96690db7a44a",
            "2c426ca993114cf8950bfa8f48d6aeda",
            "8e63a3ca70774489b50b12218effa91b",
            "221bdd5cd1714f289ab71482643ca0a2",
            "f3e98c6298d1413581602a1881eafd16",
            "b5005a77a69e4256a78a17a4a98a790f",
            "01c721b2fe8d4674a260c44b4f3b1945",
            "71fba08264684b2da3b050c9ef3b2b02",
            "34c5cfd94211424d92b11c98fdacc343",
            "c7b4b520dd4140d6b8d5a3a39c6c95a8",
            "cfec4744815541d7b6653c3ef2bbc360",
            "b92cb033185a4c42acc2f309e4d7a05b",
            "6e43078785804e57ad8e4e7aabbe18ca",
            "cfde114a88254c31b6df776a71e7f48c",
            "e2452d43bba640fb83a8e4da3651de39",
            "cf97ffb1a48f4388862ccb2373e46179",
            "2da73a0393764de99cb88bf22b5b4522",
            "eb5dc93cb2644b809093c14a7876747c",
            "9afff7605be2492fb5bd649a00c9d3cc"
          ]
        },
        "outputId": "c32062fc-26d6-49f6-af76-c9c50f191256"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/7.94k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c453e750ee44c90a9b9dc8be5d5fb43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e973bc57470b41bda53e6697d2632669"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "001290d3ed254e16b8e75eb3ecb2b901"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4b258bcb78844a085327985e96b44dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34c5cfd94211424d92b11c98fdacc343"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_gsm8k_dataset()\n",
        "\n",
        "# Select subset\n",
        "sample_dataset = create_sample_dataset(dataset, num_samples=CONFIG.benchmark_subset_size, seed=CONFIG.seed)\n",
        "\n",
        "# Display one sample\n",
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry2nMjXwsox_"
      },
      "source": [
        "### Prompt Engineering (10 points)\n",
        "\n",
        "Implement different prompting strategies for in-context learning.\n",
        "At least four of the following methods should be implemented (including baseline):\n",
        "- Zero-shot (**Baseline**)\n",
        "- Role-play prompting [[paper](https://aclanthology.org/2024.naacl-long.228/)]\n",
        "- Zero-shot CoT [[paper](https://arxiv.org/abs/2205.11916)]\n",
        "- Few-shot CoT\n",
        "- Least-to-Most prompting [[paper](https://arxiv.org/abs/2205.10625)]\n",
        "- Generated Knowledge prompting [[paper](https://aclanthology.org/2022.acl-long.225/)]\n",
        "- Any other idea to improve performance (**Optional**)\n",
        "\n",
        "Additionally, if performance exceeds 80%, **two extra points** are awarded for every 5% improvement. You can try other methods or a combination of existing ones.\n",
        "\n",
        "<a id=\"prompt-engineering\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RgBpE5vreCN"
      },
      "outputs": [],
      "source": [
        "def create_prompts(question, examples=None):\n",
        "    \"\"\"Generate various prompt types for a given question.\"\"\"\n",
        "\n",
        "    # 1. Zero-shot (Baseline)\n",
        "    zero_shot = f\"Problem: {question}\\n\\nThe answer number is \"\n",
        "\n",
        "    # 2. Role-play prompting\n",
        "    role_prompting = (\n",
        "        \"You are a brilliant math teacher helping students solve problems step-by-step.\\n\"\n",
        "        f\"Problem: {question}\\n\\nThe answer number is \"\n",
        "    )\n",
        "\n",
        "    # 3. Zero-shot Chain-of-Thought (CoT)\n",
        "    zero_shot_cot = (\n",
        "        f\"Problem: {question}\\n\"\n",
        "        \"Let's think step by step.\\n\"\n",
        "    )\n",
        "\n",
        "    # 4. Few-shot Chain-of-Thought (CoT)\n",
        "    few_shot_cot = (\n",
        "        \"Problem: If you have 3 apples and you get 2 more, how many do you have?\\n\"\n",
        "        \"Let's think step by step.\\n\"\n",
        "        \"Step 1: You start with 3 apples.\\n\"\n",
        "        \"Step 2: You get 2 more apples.\\n\"\n",
        "        \"Step 3: Total is 3 + 2 = 5.\\n\"\n",
        "        \"Answer: 5\\n\\n\"\n",
        "        f\"Problem: {question}\\n\"\n",
        "        \"Let's think step by step.\\n\"\n",
        "    )\n",
        "\n",
        "    # 5. Least-to-Most prompting\n",
        "    least_to_most = (\n",
        "        \"To solve this, break it into smaller subtasks and solve each:\\n\"\n",
        "        f\"Problem: {question}\\nSubtask 1: \"\n",
        "    )\n",
        "\n",
        "    # 6. Generated Knowledge prompting (template)\n",
        "    generated_knowledge = (\n",
        "        \"Here is some helpful background knowledge: \"\n",
        "        \"When solving math problems, identify known values, set up equations, and solve step-by-step.\\n\"\n",
        "        f\"Problem: {question}\\nLet's solve it: \"\n",
        "    )\n",
        "\n",
        "    # 7. Self-Consistency Prompting\n",
        "    self_consistency = (\n",
        "        f\"Problem: {question}\\n\"\n",
        "        \"Let's think step by step and generate multiple reasoning paths. \"\n",
        "        \"After considering these, provide the most consistent final answer.\"\n",
        "    )\n",
        "\n",
        "    # 8. Tree-of-Thought Prompting\n",
        "    tree_of_thought = (\n",
        "        f\"Problem: {question}\\n\"\n",
        "        \"Generate a tree of thought by outlining several possible intermediate reasoning steps. \"\n",
        "        \"Then select the most promising branch and provide the final answer.\"\n",
        "    )\n",
        "\n",
        "    # 9. ReAct Prompting (Reasoning and Acting)\n",
        "    react_prompt = (\n",
        "        f\"Problem: {question}\\n\"\n",
        "        \"Thought 1: Describe your initial thought process.\\n\"\n",
        "        \"Action 1: Based on Thought 1, specify an action.\\n\"\n",
        "        \"Observation 1: Note the result of that action.\\n\"\n",
        "        \"Thought 2: Reflect on the observation and plan the next step.\\n\"\n",
        "        \"Finally, provide the final answer.\"\n",
        "    )\n",
        "\n",
        "    # 10. Echo Prompting\n",
        "    echo_prompt = (\n",
        "        f\"Problem: {question}\\n\"\n",
        "        \"First, rephrase the above problem in your own words to ensure understanding.\\n\"\n",
        "        \"Then, provide a step-by-step solution followed by the final answer.\"\n",
        "    )\n",
        "\n",
        "    # 11. Skills-in-Context (SKiC) Prompting\n",
        "    skic_prompt = (\n",
        "        \"The following foundational skills are essential for solving this problem:\\n\"\n",
        "        \"- Identify key values\\n\"\n",
        "        \"- Set up relevant equations\\n\"\n",
        "        \"- Perform step-by-step reasoning\\n\"\n",
        "        f\"Problem: {question}\\n\"\n",
        "        \"Using these skills, provide a step-by-step solution and the final answer.\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"Baseline\": zero_shot,\n",
        "        \"Role-Prompting\": role_prompting,\n",
        "        \"Zero-shot CoT\": zero_shot_cot,\n",
        "        \"Few-shot CoT\": few_shot_cot,\n",
        "        \"Least-to-Most\": least_to_most,\n",
        "        \"Generated Knowledge\": generated_knowledge,\n",
        "        \"Self-Consistency\": self_consistency,\n",
        "        \"Tree-of-Thought\": tree_of_thought,\n",
        "        \"ReAct\": react_prompt,\n",
        "        \"Echo\": echo_prompt,\n",
        "        \"SKiC\": skic_prompt\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbYUEqf_svlx"
      },
      "source": [
        "### Evaluate Prompting Strategies (10 points)\n",
        "\n",
        "1. Implement an evaluation function to assess different prompts.\n",
        "2. Compare the accuracy of various prompting methods.\n",
        "3. Visualize results and show some sample responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arWJGLhnZRCk"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def extract_answer(text):\n",
        "    \"\"\"Extract the final numerical answer from the model's output.\"\"\"\n",
        "    matches = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", text)\n",
        "    return matches[-1] if matches else None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x6PthPerlWN"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_prompts(model, tokenizer, sample_dataset, seed=42):\n",
        "    \"\"\"\n",
        "    Evaluate all prompt variations on the provided sample dataset.\n",
        "    Returns:\n",
        "        accuracies (dict): Mapping of prompt_type -> accuracy score\n",
        "        all_samples (dict): Detailed results per prompt_type\n",
        "    \"\"\"\n",
        "    # For reproducibility (optional)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Suppose you have a function create_prompts(question) that returns\n",
        "    # a dict of {prompt_type: prompt_text}\n",
        "    # from your previous code:\n",
        "    # from your_prompt_file import create_prompts\n",
        "\n",
        "    # We will store detailed results and compute accuracy\n",
        "    results = defaultdict(list)  # {prompt_type: [0/1, 0/1, ...]}\n",
        "    all_samples = defaultdict(list)  # For storing question, pred, gold, etc.\n",
        "\n",
        "    for example in tqdm(sample_dataset, desc=\"Evaluating prompts\"):\n",
        "        question = example[\"question\"]  # or example[\"text\"]\n",
        "        gold_answer = example[\"answer\"]  # or example[\"label\"], must be numeric or convertible\n",
        "\n",
        "        # Generate multiple prompt variations\n",
        "        prompt_dict = create_prompts(question)\n",
        "\n",
        "        for prompt_type, prompt_text in prompt_dict.items():\n",
        "            # Tokenize and generate\n",
        "            inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "            output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Extract predicted answer\n",
        "            pred_answer = extract_answer(output_text)\n",
        "\n",
        "            # Compare with ground truth\n",
        "            if pred_answer == gold_answer:\n",
        "                results[prompt_type].append(1)\n",
        "            else:\n",
        "                results[prompt_type].append(0)\n",
        "\n",
        "            # Store details for analysis\n",
        "            all_samples[prompt_type].append({\n",
        "                \"question\": question,\n",
        "                \"prompt_text\": prompt_text,\n",
        "                \"model_output\": output_text,\n",
        "                \"pred_answer\": pred_answer,\n",
        "                \"gold_answer\": gold_answer\n",
        "            })\n",
        "\n",
        "    # Compute accuracy per prompt type\n",
        "    accuracies = {}\n",
        "    for p_type, scores in results.items():\n",
        "        accuracies[p_type] = sum(scores) / len(scores)\n",
        "\n",
        "    return accuracies, all_samples\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dP9_kZlNtBKr"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy by prompting method:\")\n",
        "accuracy, all_samples = evaluate_prompts(model, tokenizer, sample_dataset)\n",
        "\n",
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uApGU_hDuv2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLpAor8Os00J"
      },
      "outputs": [],
      "source": [
        "\n",
        "def visualize_results(model_name, accuracies):\n",
        "    \"\"\"\n",
        "    Draw a bar chart of prompt method accuracies.\n",
        "    \"\"\"\n",
        "    prompt_types = list(accuracies.keys())\n",
        "    scores = list(accuracies.values())\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.bar(prompt_types, scores, color='skyblue')\n",
        "    plt.title(f\"Prompt Method Accuracies for {model_name}\")\n",
        "    plt.xlabel(\"Prompt Method\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.ylim([0, 1])\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP5-UCT_ttKV"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZkNyO8ZklsM"
      },
      "source": [
        "# Human Preference Alignment (80 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z66fIMmWq-7o"
      },
      "source": [
        "## RLHF Flow\n",
        "\n",
        "<img src=\"https://huyenchip.com/assets/pics/rlhf/6-sft-rlhf.png\" width=\"80%\">\n",
        "\n",
        "With the rise of **ChatGPT**, **Reinforcement Learning from Human Feedback (RLHF)** has gained significant attention in both academic and industrial language modeling communities.\n",
        "\n",
        "The approach dates back to **OpenAI‚Äôs 2019 paper**:  \n",
        "[Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593).  \n",
        "\n",
        "A year later, OpenAI demonstrated RLHF‚Äôs effectiveness in **natural language generation**:  \n",
        "[Learning to Summarize from Human Feedback](https://arxiv.org/abs/2009.01325).  \n",
        "\n",
        "This research showed that fine-tuning alone leads to **suboptimal human-aligned performance**. RLHF optimizes models using human feedback, significantly improving their output quality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNOqdRXHh5XY"
      },
      "source": [
        "## Reward Models (20 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Sog7dXylZT9"
      },
      "source": [
        "### Question 2 (5 points):\n",
        "<img width=\"50%\" alt=\"image\" src=\"https://github.com/RLHFlow/RLHFlow.github.io/blob/main/assets/BT-and-Pref-RMs.png?raw=true\">\n",
        "\n",
        "In Reinforcement Learning from Human Feedback (RLHF), the reward model is essential for aligning large language models with human preferences. A widely used method, based on the **Bradley-Terry** model, trains the reward model using the following pairwise ranking loss function for a prompt and two responses (<font color='green'><b>chosen</b></font> and <font color='red'><b>rejected</b></font>):\n",
        "\n",
        "$$\n",
        "\\text{loss}(r_{\\theta}) = -\\mathbb{E}_{(x, y_0, y_1, i) \\sim D} \\left[ \\log \\left( \\sigma \\left( r_{\\theta}(x, y_i) - r_{\\theta}(x, y_{1-i}) \\right) \\right) \\right]\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $x$ is the prompt,\n",
        "- $y_0$ and $y_1$ are two responses,\n",
        "- $i$ (0 or 1) indicates the human-preferred response,\n",
        "- $r_{\\theta}(x, y)$ is the reward model‚Äôs scalar value for the prompt $ x $ and the response $ y $,\n",
        "- $\\sigma$ is the sigmoid function.\n",
        "\n",
        "**a)** How this loss function encourages higher scores for preferred responses.\n",
        "\n",
        "**b)** Discuss one potential limitation of this approach, such as reward hacking (e.g., favoring longer responses), and suggest a general strategy to mitigate it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRAs9mMwhisg"
      },
      "source": [
        "Below is a detailed answer addressing both parts of the question‚Äî(a) how the Bradley-Terry‚Äìstyle loss encourages higher reward for preferred responses, and (b) potential pitfalls such as reward hacking, along with mitigation strategies.\n",
        "\n",
        "---\n",
        "\n",
        "## **(a) How the Loss Function Encourages Higher Scores for Preferred Responses**\n",
        "\n",
        "1. **Pairwise Comparison**  \n",
        "   The loss function in question is:\n",
        "   \\[\n",
        "   \\mathcal{L}(r_\\theta) = -\\log\\bigl(\\sigma\\bigl(r_\\theta(z, y_c) - r_\\theta(z, y_r)\\bigr)\\bigr),\n",
        "   \\]\n",
        "   where\n",
        "   - $z$ is the prompt,\n",
        "   - $y_c$ is the human-preferred (chosen) response,\n",
        "   - $y_r$ is the less-preferred (rejected) response,\n",
        "   - $r_\\theta$ is the reward model‚Äôs scalar output,\n",
        "   - $\\sigma(\\cdot)$ is the sigmoid (logistic) function.\n",
        "\n",
        "2. **Reward Difference**  \n",
        "   Inside the sigmoid, we have $r_\\theta(z, y_c) - r_\\theta(z, y_r)$. The sigmoid $\\sigma(x)$ increases monotonically with $x$. Therefore, to maximize $\\sigma\\bigl(r_\\theta(z, y_c) - r_\\theta(z, y_r)\\bigr)$, the model must **increase** the difference $r_\\theta(z, y_c)\\;-\\;r_\\theta(z, y_r)$.\n",
        "\n",
        "3. **Minimizing Negative Log-Likelihood**  \n",
        "   By **minimizing** $-\\log(\\sigma(\\dots))$, the model is effectively trying to make  \n",
        "   \\[\n",
        "   r_\\theta(z, y_c) \\;>\\; r_\\theta(z, y_r)\n",
        "   \\]\n",
        "   with as large a margin as possible. The larger $r_\\theta(z, y_c)$ is relative to $r_\\theta(z, y_r)$, the lower the loss.\n",
        "\n",
        "4. **Interpretation**  \n",
        "   - If $y_c$ is consistently assigned a higher reward than $y_r$, the loss is low.  \n",
        "   - If $y_c$ and $y_r$ have similar rewards (or if $y_r$ is assigned a higher reward), the loss is high, pushing the model to adjust its parameters so that the chosen response scores higher.\n",
        "\n",
        "Hence, this **pairwise ranking approach** enforces that the reward model learn to **rank ‚Äúpreferred‚Äù responses above ‚Äúrejected‚Äù responses**, aligning the model‚Äôs internal reward function with human preference judgments.\n",
        "\n",
        "---\n",
        "\n",
        "## **(b) Potential Limitations (‚ÄúReward Hacking‚Äù) & Mitigation**\n",
        "\n",
        "1. **Reward Hacking / Spurious Correlations**  \n",
        "   - **Definition**: ‚ÄúReward hacking‚Äù occurs when the model (or downstream policy) discovers **shortcuts** to increase its predicted reward without genuinely improving quality or alignment.  \n",
        "   - **Examples**:  \n",
        "     - **Longer responses**: The model might learn that, on average, annotators prefer more detailed answers, so it inflates the length of its output without truly improving content.  \n",
        "     - **Overuse of keywords**: If certain phrases or words (e.g., ‚ÄúIn summary,‚Äù ‚ÄúI understand‚Äù) correlate with higher human preference, the model might overuse them.\n",
        "\n",
        "2. **Relative (Not Absolute) Calibration**  \n",
        "   - This Bradley-Terry approach only ensures the **chosen** response is *scored higher* than the *rejected* one, but it doesn‚Äôt necessarily **calibrate** absolute reward values. The model might not learn how ‚Äúgood‚Äù an answer is in an absolute sense, only that it is better than a particular alternative.\n",
        "\n",
        "3. **Mitigation Strategies**  \n",
        "   - **Diverse Training Data**: Provide a wide variety of prompts and responses (including negative examples) so the model can‚Äôt exploit narrow correlations.  \n",
        "   - **Regularization & Penalties**:  \n",
        "     - **Length penalty** or constraints to prevent artificially long outputs.  \n",
        "     - **Coverage / repetition penalties** to discourage repetitive or ‚Äúkeyword-stuffing‚Äù behavior.  \n",
        "   - **Human-in-the-Loop**: Periodically review outputs for signs of reward hacking and revise labeling or reward signals accordingly.  \n",
        "   - **Multiple Objectives**: Combine preference ranking with other objectives (e.g., factual correctness, brevity, style constraints) to balance different aspects of a ‚Äúgood‚Äù response.  \n",
        "   - **Calibrated Reward Modeling**: Some methods attempt to produce a reward that‚Äôs not purely relative but also has an absolute scale‚Äîe.g., learning from multiple reference points or from scalar quality scores (if available).\n",
        "\n",
        "By incorporating these mitigation steps, one can reduce the likelihood of the reward model learning superficial features instead of truly aligned, high-quality responses.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "\n",
        "- The **Bradley-Terry‚Äìstyle pairwise loss** pushes the reward model to assign **higher scores** to the human-preferred response.  \n",
        "- A key **limitation** is that this is a **relative** ranking signal, which can lead to ‚Äúreward hacking‚Äù if the model learns to exploit superficial correlations.  \n",
        "- **Mitigations** include broader data coverage, explicit penalties for unwanted behavior, multi-objective training, and regular human oversight."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWmCEsDvkinK"
      },
      "source": [
        "### Question 3 (5 points):\n",
        "\n",
        "The Bradley-Terry model is widely used in RLHF to train reward models by converting pairwise human preferences into a single scalar value. However, this approach has limitations when capturing complex human values like helpfulness, honesty, and safety, which may require multiple dimensions.\n",
        "\n",
        "**a)** Why a single scalar reward might fail to capture trade-offs between objectives like helpfulness and safety, using a concrete example (e.g., a response to a user query).\n",
        "\n",
        "**b)** Describe one alternative method to the Bradley-Terry model that addresses these limitations, such as by considering multiple objectives, mitigating biases, or improving interpretability. (For inspiration, explore resources like this [repository](https://github.com/RLHFlow/RLHF-Reward-Modeling/) or this [paper](https://arxiv.org/abs/2406.12845)). How does this alternative improve upon the single-scalar approach?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX34y9qu_eby"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## (a) Why a Single Scalar Reward Can Fail for Complex Values\n",
        "\n",
        "1. **Multiple Human Values, One Number**  \n",
        "   Bradley-Terry‚Äìstyle models produce a single scalar reward per response. This scalar is supposed to represent how ‚Äúgood‚Äù a response is overall. However, human preferences often span **several dimensions** (e.g., helpfulness, safety, honesty, politeness). Compressing them into a **single** number can lose critical information.\n",
        "\n",
        "2. **Trade-Offs Are Not Explicit**  \n",
        "   With a single scalar, the model has no direct way to see or adjust the **trade-off** between, say, a highly helpful but slightly unsafe response vs. a moderately helpful but very safe response. Both might map to a similar single reward, making it unclear which dimension the model should prioritize.\n",
        "\n",
        "3. **Concrete Example**  \n",
        "   - **User Query**: ‚ÄúHow can I invest in stocks to get the highest returns quickly?‚Äù  \n",
        "   - A single-scalar reward might end up favoring a response that is extremely detailed (hence ‚Äúhelpful‚Äù) but also includes risky or borderline illegal strategies (thus ‚Äúunsafe‚Äù).  \n",
        "   - Alternatively, a single-scalar approach might push the model to produce safe but very vague answers, sacrificing real helpfulness.  \n",
        "   - Because all these aspects (helpfulness, safety, compliance) get collapsed into one number, the model cannot explicitly balance them.  \n",
        "\n",
        "Hence, single-scalar rewards risk ignoring nuanced dimensions of human values.\n",
        "\n",
        "---\n",
        "\n",
        "## (b) One Alternative: Multi-Dimensional or Multi-Objective Reward Models\n",
        "\n",
        "**Overview**  \n",
        "An alternative is to **model multiple objectives** in parallel‚Äî**helpfulness**, **safety**, **honesty**, etc.‚Äîand produce a **vector of reward signals** instead of a single scalar. Below is one example approach:\n",
        "\n",
        "1. **Multi-Dimensional Reward Vector**  \n",
        "   - **Reward Model Architecture**: Instead of a single output head (like Bradley-Terry‚Äôs scalar), the reward model has multiple heads, each corresponding to a specific dimension (e.g., helpfulness head, safety head, correctness head).  \n",
        "   - **Training**: You collect preference data or labeled data for each dimension. For instance, you might have separate labels for how ‚Äúhelpful‚Äù a response is vs. how ‚Äúsafe‚Äù it is. The model then learns to predict each dimension‚Äôs score.\n",
        "\n",
        "2. **Aggregating Objectives**  \n",
        "   - **Weighted Combination**: At policy training time (e.g., RL or direct optimization), you can combine these dimension-specific scores with **weights** that reflect your priorities. For instance, you might set a higher weight on safety if that is non-negotiable.  \n",
        "   - **Constraint-Based**: Alternatively, you can treat certain dimensions (like safety) as a **hard constraint** that must exceed a threshold, while optimizing others (like helpfulness) more freely.\n",
        "\n",
        "3. **Benefits**  \n",
        "   - **Explicit Trade-Offs**: You can tune how much to prioritize each dimension. If you see the policy being too ‚Äúsafe‚Äù at the expense of helpfulness, you adjust the weights or constraints.  \n",
        "   - **Interpretability**: You get more insight into *why* a model‚Äôs response is considered high- or low-quality‚Äîbecause you can see separate scores for each dimension.  \n",
        "   - **Mitigating Biases & ‚ÄúReward Hacking‚Äù**: By monitoring multiple signals, you reduce the chance of the model exploiting one dimension at the expense of others. For example, if the model tries to produce very long answers (to appear ‚Äúhelpful‚Äù), it might get penalized on a ‚Äúrelevance‚Äù or ‚Äúconciseness‚Äù dimension.\n",
        "\n",
        "4. **Real-World Examples**  \n",
        "   - **Constitutional AI** (Anthropic): Uses a set of guiding principles (safety, helpfulness, etc.) and effectively enforces them via multi-step feedback or multi-objective constraints.  \n",
        "   - **Multi-Objective RL**: Some labs maintain separate ‚Äúaxes‚Äù for alignment vs. performance, then fine-tune to find Pareto-optimal solutions that balance these axes.  \n",
        "   - **Redwood Research**: Focuses on specific constraints (e.g., removing violent content) while preserving task performance, effectively turning alignment into a multi-objective optimization problem (no ‚Äúone-size-fits-all‚Äù scalar).\n",
        "\n",
        "**How It Improves Over Single-Scalar**  \n",
        "- Instead of forcing the model to condense all human values into a single dimension, a **multi-objective** or **multi-dimensional** approach gives the model direct feedback on each relevant axis. This avoids the pitfalls of single-scalar compression, allows for explicit **trade-offs**, and provides more **transparent** signals for the model and for human operators.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "\n",
        "- **Single-scalar Bradley-Terry** approaches can fail to capture nuanced trade-offs among multiple values (helpfulness, safety, honesty).  \n",
        "- **Multi-dimensional** reward modeling (or multi-objective RL) **preserves** these dimensions, enabling better balancing, interpretability, and mitigating reward hacking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7WKTlsevH8I"
      },
      "source": [
        "---\n",
        "\n",
        "**Find More:**\n",
        "<br>[RewardBench LeaderBoard](https://huggingface.co/learn/deep-rl-course/en/unit0/introduction)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZkpabXSl7q4"
      },
      "source": [
        "### Inference from the Reward Model (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ied3eJK0EX0H"
      },
      "source": [
        "<div align=\"center\"><img width=\"90%\" alt=\"image\" src=\"https://github.com/Nicolinho/QRM/blob/main/assets/method_vis.png?raw=true\"></div>\n",
        "\n",
        "**Quantile Reward Models (QRM)** generates a distribution over rewards by aggregating individual distributions over attribute scores like helpfulness and harmlessness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j38krUM6xmCa"
      },
      "source": [
        "- Load the [reward model](https://huggingface.co/nicolinho/QRM-Llama3.1-8B-v2) and its tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cGFpVCPIWmvD"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Replace with actual reward model name, e.g., \"OpenAssistant/reward-model-deberta-v3-large\"\n",
        "reward_model_name = \"OpenAssistant/reward-model-deberta-v3-large\"\n",
        "\n",
        "# Load the reward model and tokenizer\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_name)\n",
        "reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETBOFl7rynjz"
      },
      "source": [
        "- Generate reward scores for both responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CI06X2yJheyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b6a7258-0fb1-48ae-d8d7-c1827e986871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen Response Score: 4.3622\n",
            "Rejected Response Score: 1.9841\n"
          ]
        }
      ],
      "source": [
        "# Prompt and responses\n",
        "sample_prompt = \"Do wooden pencils contain lead as their core?\"\n",
        "chosen_response = \"No, wooden pencils do not contain lead in their core. The term \\\"lead\\\" is a misnomer, as wooden pencils actually use graphite for their core. Graphite was historically called \\\"black lead\\\" due to its appearance, leading to the common misconception that pencils contain lead.\"\n",
        "rejected_response = \"Yes, wooden pencils typically contain a core made of graphite and clay, which is commonly referred to as \\\"lead\\\" despite not being made of actual lead.\"\n",
        "\n",
        "\n",
        "def get_reward_score(prompt, response):\n",
        "    inputs = reward_tokenizer(prompt + response, return_tensors=\"pt\")\n",
        "\n",
        "    # Move input tensors to same device as model\n",
        "    device = next(reward_model.parameters()).device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = reward_model(**inputs)\n",
        "\n",
        "    return outputs.logits.squeeze().tolist()\n",
        "\n",
        "\n",
        "# Generate reward scores\n",
        "chosen_score = get_reward_score(sample_prompt, chosen_response)\n",
        "rejected_score = get_reward_score(sample_prompt, rejected_response)\n",
        "\n",
        "print(f\"Chosen Response Score: {chosen_score:.4f}\")\n",
        "print(f\"Rejected Response Score: {rejected_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0g5d58WrSaI"
      },
      "source": [
        "- Visualize the results:\n",
        "\n",
        "    + Create a bar chart comparing the reward scores of the chosen vs. the rejected response for each attribute.\n",
        "    + Overlay a line chart representing the gating output coefficients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XkkyvEBKrQPZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "f4fd2e3e-6f89-4286-e258-6ea4fafbc0d8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Reward attributes (from model output or mock values for example)\n",
        "attributes = ['helpfulness', 'correctness', 'coherence', 'complexity', 'verbosity']\n",
        "\n",
        "# Example scores (replace these with actual reward scores from your model)\n",
        "chosen_scores = [0.8, 0.9, 0.85, 0.7, 0.6]\n",
        "rejected_scores = [0.5, 0.6, 0.55, 0.4, 0.3]\n",
        "gating_coeffs = [0.25, 0.3, 0.2, 0.15, 0.1]  # Optional: if you have gating output\n",
        "\n",
        "# Set positions\n",
        "x = np.arange(len(attributes))\n",
        "width = 0.35\n",
        "\n",
        "# Create bar plot\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "bars1 = ax1.bar(x - width/2, chosen_scores, width, label='Chosen', color='tab:blue')\n",
        "bars2 = ax1.bar(x + width/2, rejected_scores, width, label='Rejected', color='tab:red')\n",
        "\n",
        "# Add labels and titles\n",
        "ax1.set_ylabel('Reward Score')\n",
        "ax1.set_title('Reward Score Comparison by Attribute')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(attributes)\n",
        "ax1.legend(loc=\"upper left\")\n",
        "\n",
        "# Optional: Add line plot for gating coefficients (secondary axis)\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(x, gating_coeffs, label='Gating Coefficient', color='green', marker='o', linestyle='--')\n",
        "ax2.set_ylabel('Gating Coefficient')\n",
        "ax2.legend(loc=\"upper right\")\n",
        "\n",
        "# Annotate bar heights\n",
        "for bar in bars1 + bars2:\n",
        "    height = bar.get_height()\n",
        "    ax1.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                 xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka51QlNlkCgo"
      },
      "source": [
        "## PPO (15 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVTtkH-XkG08"
      },
      "source": [
        "### Question 4 (5 points):\n",
        "**a)** Describe the Proximal Policy Optimization (PPO) algorithm and explain its role in the Reinforcement Learning from Human Feedback (RLHF) framework.\n",
        "\n",
        "**b)** Specifically, is PPO an on-policy or off-policy algorithm, and why is this characteristic important for its application in RLHF?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr50YXvFkp5D"
      },
      "source": [
        "**(a) Description of the Proximal Policy Optimization (PPO) Algorithm and Its Role in RLHF**\n",
        "\n",
        "Proximal Policy Optimization (PPO) is a policy-gradient method designed to update an agent‚Äôs policy in a **stable and sample-efficient** manner . Rather than allowing large, unconstrained steps in the policy parameters, PPO imposes a **clipped objective** that restricts how far the new policy can deviate from the old policy. This clipping prevents destructive updates that could degrade performance or destabilize training.\n",
        "\n",
        "In **Reinforcement Learning from Human Feedback (RLHF)**, a reward model is trained (or reused) to encode human preferences for the agent‚Äôs outputs. The policy (in this context, typically a language model) interacts with the environment by generating text, receives a reward from the reward model, and then updates its parameters to maximize this reward. PPO‚Äôs **role** is to ensure that each update is sufficiently large to incorporate the new feedback signal yet **not so large** that the policy drifts drastically from previously successful behaviors. As a result, PPO helps maintain **training stability** and **consistent performance** while aligning the model‚Äôs outputs with human preferences.\n",
        "\n",
        "---\n",
        "\n",
        "**(b) On-Policy or Off-Policy Nature of PPO and Its Importance in RLHF**\n",
        "\n",
        "PPO is **an on-policy algorithm** [1]. This designation implies that the data (state‚Äìaction pairs and rewards) used to update the policy must be generated by **the same policy** currently being trained. In other words, the model collects new experiences (e.g., text completions and their corresponding rewards) based on the most recent version of the policy, and those experiences guide the next update step.\n",
        "\n",
        "In the **RLHF setting**, being on-policy is critical because the reward signals from human feedback‚Äîor from a learned reward model‚Äîmust match the policy‚Äôs present output style. If experiences were gathered from an older or different policy (off-policy), there would be a **distribution mismatch** between the data used to compute gradients and the policy being updated. This mismatch can lead to **suboptimal or unstable** learning. By maintaining an on-policy approach, PPO ensures that the collected data accurately reflects the current policy‚Äôs behavior, thereby producing more reliable gradient estimates and promoting better alignment of the model with human-defined preferences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45WO6uGkkuRA"
      },
      "source": [
        "### Question 5 (5 points):\n",
        "\n",
        "**a)** Why is it crucial to prevent drastic changes in the Large Language Model's policy during the PPO optimization process?\n",
        "\n",
        "**b)** Explain how PPO addresses the risk of overoptimization or instability in the context of aligning LLMs with human preferences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIhAD8BM1CbA"
      },
      "source": [
        "**(a) Importance of Preventing Drastic Changes in the Large Language Model‚Äôs Policy During PPO**\n",
        "\n",
        "Large Language Models (LLMs) undergoing Reinforcement Learning (RL) updates, particularly with human feedback signals, are prone to performance degradation if the policy is altered too aggressively. In Proximal Policy Optimization (PPO) [1], *drastic changes* refer to large parameter updates that shift the policy far from its previous distribution. Such abrupt shifts can lead to:\n",
        "\n",
        "1. **Catastrophic Forgetting**: Overly large updates may cause the model to forget previously learned linguistic skills, adversely affecting its generative quality and coherence.  \n",
        "2. **Reward Hacking**: When the policy changes abruptly, the LLM may exploit reward function loopholes, generating text that superficially maximizes reward but fails to align with human preferences (e.g., producing repetitive or off-topic content).  \n",
        "3. **Training Instability**: Sudden policy changes can introduce variance in gradient estimates and cause oscillations in training, making it difficult to converge to a stable solution.  \n",
        "\n",
        "By constraining updates, PPO ensures that each iteration refines the LLM‚Äôs behavior without discarding its existing language capabilities or generating degenerate text outputs.\n",
        "\n",
        "---\n",
        "\n",
        "**(b) How PPO Addresses Overoptimization or Instability When Aligning LLMs with Human Preferences**\n",
        "\n",
        "PPO addresses overoptimization and instability via its *clipped objective* mechanism [1]. The policy update ratio, defined by \\(\\frac{\\pi_\\theta(a \\mid s)}{\\pi_{\\theta_\\text{old}}(a \\mid s)}\\), is restricted within a certain range (e.g., \\([1 - \\epsilon,\\, 1 + \\epsilon]\\)). This clipping provides:\n",
        "\n",
        "1. **Controlled Policy Update**: By limiting the deviation between the old and new policies, PPO prevents the LLM from over-fitting to the reward model or from abruptly discarding beneficial linguistic behaviors.  \n",
        "2. **Balanced Exploration and Exploitation**: The clipped objective allows the LLM to steadily improve its alignment with human feedback while maintaining enough exploration to avoid local maxima.  \n",
        "3. **Stable Convergence**: The PPO update rule mitigates large gradients that could destabilize training, ensuring more consistent progress toward better alignment rather than erratic oscillations in model outputs.  \n",
        "\n",
        "Thus, PPO‚Äôs clipping strategy is critical for *aligning large-scale language models with human preferences* in a stable, incremental fashion. It keeps the policy updates in check, minimizing the risk of sudden, detrimental shifts in the model‚Äôs distribution while steadily improving the quality and alignment of generated text.\n",
        "\n",
        "---\n",
        "\n",
        "**Reference**  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scEGGwT91bG5"
      },
      "source": [
        "### Question 6 (5 points):\n",
        "\n",
        "Consider the following simplified form of PPO's objective function used in RLHF:\n",
        "\n",
        "$$\n",
        "\\text{objective}(\\phi) = \\mathbb{E}_{(x,y) \\sim D_{\\pi_{\\phi}^{\\text{RL}}}} \\left[ r_{\\theta}(x, y) - \\beta \\log \\left( \\frac{\\pi_{\\phi}^{\\text{RL}}(y \\mid x)}{\\pi^{\\text{SFT}}(y \\mid x)} \\right) \\right] + \\gamma \\mathbb{E}_{x \\sim D_{\\text{pretrain}}} \\left[ \\log(\\pi_{\\phi}^{\\text{RL}}(x)) \\right]\n",
        "$$\n",
        "\n",
        "**a)** Why does the reward term, $r_{\\theta}(x, y)$ , appear in this objective function even though we are differentiating with respect to the policy parameters, $\\phi$?\n",
        "\n",
        "**b)** What is the role of this term in driving the policy improvement?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzeuZttF2vfz"
      },
      "source": [
        "**(a) Why does the reward term $r_\\theta(z,y)$ appear in this objective function even though we are differentiating with respect to the policy parameters $\\phi$?**  \n",
        "\n",
        "In the RLHF setting, the reward function $r_\\theta(z,y)$ (often provided by a separate reward model or learned from human feedback) is **evaluated on the outputs** $y$ that the policy (parameterized by $\\phi$) generates from inputs $z$. Even though we do not backpropagate through the reward model‚Äôs parameters $\\theta$ (i.e., we are not updating $\\theta$ in this step), the reward value itself is still **used as a scalar signal** guiding how we adjust the policy parameters $\\phi$. Concretely:\n",
        "\n",
        "1. **No Gradient Through $\\theta$:** The reward model $\\theta$ is usually kept fixed at this stage, so there is no direct gradient flow through $r_\\theta$. However, the reward‚Äôs numerical value influences the policy gradient step.  \n",
        "2. **Policy-Dependent Outputs:** The policy $\\pi_\\phi$ determines which outputs $y$ are sampled; thus the distribution of data that enters the reward function $r_\\theta$ depends on $\\phi$. As a result, when we compute the policy gradient, the reward signals from $r_\\theta$ shape the direction in which $\\phi$ is updated.\n",
        "\n",
        "Hence, $r_\\theta$ remains in the objective to **evaluate** each policy-sampled output $y$ for quality or alignment, even though $\\theta$ itself is not being updated. The policy parameters $\\phi$ are tuned so that the policy **prefers** sequences $y$ that yield higher reward.\n",
        "\n",
        "---\n",
        "\n",
        "**(b) What is the role of this term in driving policy improvement?**\n",
        "\n",
        "The reward term $r_\\theta(z,y)$ serves as the **primary optimization target** that the policy attempts to maximize. In the context of RLHF, it captures human-aligned preferences, such as helpfulness or correctness. Its role is twofold:\n",
        "\n",
        "1. **Shaping the Policy Toward Better Outputs:** By **increasing** the probability of outputs $y$ that yield higher $r_\\theta$ and **decreasing** the probability of those with lower $r_\\theta$, the policy iteratively shifts toward generating text that aligns with the reward model‚Äôs notion of ‚Äúgood‚Äù behavior.  \n",
        "2. **Ensuring Alignment with Human Feedback:** Because $r_\\theta$ is learned from (or directly encodes) human judgments, it **steers** the large language model‚Äôs policy parameters $\\phi$ toward outputs that humans prefer, thereby **improving** the model‚Äôs alignment with user needs or ethical constraints.\n",
        "\n",
        "In summary, the reward term in the PPO objective is the **driving force** that incentivizes policy improvement. Even though the reward model‚Äôs parameters $\\theta$ are not updated in this step, the reward signal itself is crucial for guiding how the policy $\\phi$ should evolve to produce more desirable outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_CkMu18uleE"
      },
      "source": [
        "---\n",
        "**Learn More:**\n",
        "<br>[Huggingface Deep Reinforcement Learning Course](https://huggingface.co/learn/deep-rl-course/en/unit0/introduction)\n",
        "<br>[Research Papers for Reinforcement Learning with Human Feedback ](https://github.com/opendilab/awesome-RLHF)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZR7dUWP2cdg"
      },
      "source": [
        "## DPO (25 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbmh23R02d4V"
      },
      "source": [
        "### Question 7 (5 points):\n",
        "<div align=\"center\"><img width=\"80%\" alt=\"image\" src=\"https://miro.medium.com/v2/resize:fit:1400/1*GZnOKpza5yE616uN4OlaVg.jpeg\"></div>\n",
        "\n",
        "**a)** How does Direct Preference Optimization (DPO) differ from RLHF in aligning LLMs? Explain the DPO loss function below and its key terms:\n",
        "\n",
        "$$\n",
        "\\text{L}_{\\text{DPO}}(\\pi_\\theta; \\pi_{\\text{ref}}) = -\\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w | x)}{\\pi_{\\text{ref}}(y_w | x)} - \\beta \\log \\frac{\\pi_\\theta(y_l | x)}{\\pi_{\\text{ref}}(y_l | x)} \\right) \\right]\n",
        "$$\n",
        "\n",
        "**b)** What is the role of the $ \\pi_{\\text{ref}} $ in the DPO loss function, and why is it necessary for stable training?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE0zRF_V26Ag"
      },
      "source": [
        "**(a) How Does Direct Preference Optimization (DPO) Differ from RLHF in Aligning LLMs?**\n",
        "\n",
        "Direct Preference Optimization (DPO) is a method that **directly** optimizes a language model‚Äôs parameters to reflect pairwise human preferences, without requiring a separate reward model or an explicit KL regularization term (as used in PPO-based RLHF). The core idea is:\n",
        "\n",
        "1. **Pairwise Comparisons**: For an input $z$, we have two candidate outputs: a ‚Äúpreferred‚Äù sample $y^+$ and a ‚Äúdispreferred‚Äù sample $y^-$. These come from human annotations or a preference dataset.  \n",
        "2. **Log-Likelihood Ratio**: The DPO objective compares the model‚Äôs log-probability of $y^+$ vs. $y^-$ under the same context $z$. By **increasing** $\\log \\frac{\\pi_{\\theta_{\\text{net}}}(y^+|z)}{\\pi_{\\theta_{\\text{net}}}(y^-|z)}$, the model learns to favor outputs that humans prefer.  \n",
        "3. **No Separate Reward Model**: Unlike standard RLHF, which uses a reward model and a policy optimization step (e.g., PPO), DPO treats preference data as direct supervision on the **relative** probabilities of pairs $(y^+, y^-)$.  \n",
        "4. **$\\beta$-Scaled Comparison**: A scaling factor $\\beta$ is applied inside the log function to modulate the strength of the preference signal. This factor helps control the magnitude of updates and the sensitivity to differences in log-likelihood.\n",
        "\n",
        "Overall, **DPO** can be viewed as a simpler, more direct approach to preference alignment than RLHF with PPO. Instead of iterating between a reward model and policy updates, DPO **directly** shifts the model distribution so that **preferred samples** become more likely.\n",
        "\n",
        "---\n",
        "\n",
        "**(b) Role of $\\pi_{\\theta_{\\text{net}}}$ in the DPO Loss and Why It‚Äôs Necessary for Stable Training**\n",
        "\n",
        "1. **$\\pi_{\\theta_{\\text{net}}}$ as the Optimized Distribution**  \n",
        "   - $\\pi_{\\theta_{\\text{net}}}(y|z)$ denotes the model‚Äôs probability of generating output $y$ given input $z$. This **policy** (or distribution) is the main object of optimization in DPO.  \n",
        "   - By adjusting $\\theta_{\\text{net}}$, we change how likely the model is to produce $y^+$ relative to $y^-$ for each context $z$.\n",
        "\n",
        "2. **Ensuring Stability via Direct Comparison**  \n",
        "   - DPO‚Äôs objective uses the ratio $\\frac{\\pi_{\\theta_{\\text{net}}}(y^+|z)}{\\pi_{\\theta_{\\text{net}}}(y^-|z)}$, which ties **all** updates to pairwise preferences.  \n",
        "   - This direct ratio-based update avoids large swings that might occur if we tried to push up $y^+$ or push down $y^-$ independently. Instead, **both** outputs are evaluated in the same context, stabilizing the gradient signal.  \n",
        "   - The scaling factor $\\beta$ further ensures that changes to $\\pi_{\\theta_{\\text{net}}}$ remain **controlled** and do not explode when preferences are strong.\n",
        "\n",
        "3. **Why It‚Äôs Necessary**  \n",
        "   - In preference-based alignment, the **model itself** ($\\pi_{\\theta_{\\text{net}}}$) must incorporate humans‚Äô relative judgments of outputs.  \n",
        "   - If the distribution were not directly updated via these pairwise comparisons, it could lead to **inconsistent** or **unstable** optimization (e.g., over-correcting for certain preferences or ignoring them entirely).  \n",
        "   - By keeping $\\pi_{\\theta_{\\text{net}}}$ in the loop, DPO ensures that each gradient step is grounded in actual differences between **preferred** and **dispreferred** outputs, promoting **smooth convergence** toward a model that aligns with human-labeled preferences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh5So3euzEL2"
      },
      "source": [
        "### Load Model & Tokenizer (2.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3n6iCFe-2dbx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e133e78-f574-4044-dcff-b019a15a026f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model and tokenizer using unsloth...\n",
            "==((====))==  Unsloth 2025.3.18: Fast Llama patching. Transformers: 4.49.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "def load_model_and_tokenizer(model_id, max_seq_length):\n",
        "    print(\"Loading model and tokenizer using unsloth...\")\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = model_id,\n",
        "        max_seq_length = max_seq_length,\n",
        "    )\n",
        "\n",
        "    tokenizer = get_chat_template(tokenizer, \"llama-3\")\n",
        "\n",
        "    return model, tokenizer\n",
        "model, tokenizer = load_model_and_tokenizer(\n",
        "    model_id=CONFIG.model_name,\n",
        "    max_seq_length=CONFIG.max_seq_length\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bitsandbytes as bnb\n",
        "print(bnb.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rrsyhdGbm_C",
        "outputId": "84682953-cf6b-4474-bc36-884389e0e863"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.45.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67ax_PdzcLw"
      },
      "source": [
        "### Preparing Data (2.5 points)\n",
        "- Load dataset for training.\n",
        "- Convert data into the expected format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wQ03qXqky_pJ"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(CONFIG.dataset_name, split='train')\n",
        "\n",
        "def filter_responses(row, similarity_threshold=0.6, word_limit=1000):\n",
        "    chosen_text = row['chosen'][-1]['content'] if isinstance(row['chosen'], list) else row['chosen']\n",
        "    rejected_text = row['rejected'][-1]['content'] if isinstance(row['rejected'], list) else row['rejected']\n",
        "\n",
        "    # Compute similarity score\n",
        "    similarity = ratio(chosen_text, rejected_text)\n",
        "\n",
        "    # Count words in each response\n",
        "    chosen_word_count = len(chosen_text.split())\n",
        "    rejected_word_count = len(rejected_text.split())\n",
        "\n",
        "    # Apply filtering conditions\n",
        "    if similarity >= similarity_threshold:  # Remove if too similar\n",
        "        return False\n",
        "    if chosen_word_count >= word_limit or rejected_word_count >= word_limit:  # Remove if too long\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# Apply filtering\n",
        "dataset = dataset.filter(filter_responses)\n",
        "\n",
        "# Select a subset\n",
        "dataset = dataset.shuffle(seed=CONFIG.seed).select(range(CONFIG.train_data_size))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG.dataset_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "WeKl0foTbNHg",
        "outputId": "48c2dcaa-c466-4501-8913-a7e66daad246"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mlabonne/orpo-dpo-mix-40k'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG.train_data_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycl0G_RYzCvn",
        "outputId": "a2c43fab-5542-4962-b669-753bacfc95c3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1600"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ3Kky4vzEr6",
        "outputId": "7ee2fee2-9c74-4bd6-85f0-f71275fd5c02"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1600"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1Lxtpx_3rxBY"
      },
      "outputs": [],
      "source": [
        "def format_dpo_dataset(example):\n",
        "    prompt = example[\"prompt\"] if \"prompt\" in example else example[\"question\"]\n",
        "    chosen_response = example[\"chosen\"]\n",
        "    rejected_response = example[\"rejected\"]\n",
        "\n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "        \"chosen\": chosen_response,\n",
        "        \"rejected\": rejected_response\n",
        "    }\n",
        "\n",
        "columns_to_remove = [col for col in [\"source\", \"question\", \"chosen\", \"rejected\"]\n",
        "                     if col in dataset.column_names]\n",
        "\n",
        "dataset = dataset.map(\n",
        "    format_dpo_dataset,\n",
        "    num_proc=12,\n",
        "    remove_columns=columns_to_remove,\n",
        "    desc=\"Formatting dataset for DPO training\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFRcnvyLz8pc"
      },
      "source": [
        "### Applying LoRA Adapters (2.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VI6bNnlRzhxx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e8b7c1-0cd4-451e-ff57-ef4308e47716"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
            "Unsloth 2025.3.18 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0.05,  # ÿß€åŸÜ ÿ®ÿßÿπÿ´ Ÿáÿ¥ÿØÿßÿ± performance ŸÖ€åÿ¥Ÿáÿå Ÿæÿß€å€åŸÜ ÿ™Ÿàÿ∂€åÿ≠ ÿØÿßÿØŸÖ\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    bias = \"none\",\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVT8N9891Ubw"
      },
      "source": [
        "### Train the Model (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gqpZTuCny_f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e9e8787-892a-4091-c55f-45cebd0e8d96"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,600 | Num Epochs = 3 | Total steps = 1,200\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 2\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 2 x 1) = 4\n",
            " \"-____-\"     Trainable parameters = 9,175,040/3,000,000,000 (0.31% trained)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtahamaj4\u001b[0m (\u001b[33mtahamaj4-srg-ssr\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250322_061837-ho0tbm1m</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tahamaj4-srg-ssr/huggingface/runs/ho0tbm1m' target=\"_blank\">trainer_output</a></strong> to <a href='https://wandb.ai/tahamaj4-srg-ssr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/tahamaj4-srg-ssr/huggingface' target=\"_blank\">https://wandb.ai/tahamaj4-srg-ssr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/tahamaj4-srg-ssr/huggingface/runs/ho0tbm1m' target=\"_blank\">https://wandb.ai/tahamaj4-srg-ssr/huggingface/runs/ho0tbm1m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='68' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  68/1200 11:56 < 3:24:54, 0.09 it/s, Epoch 0.17/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>rewards / chosen</th>\n",
              "      <th>rewards / rejected</th>\n",
              "      <th>rewards / accuracies</th>\n",
              "      <th>rewards / margins</th>\n",
              "      <th>logps / chosen</th>\n",
              "      <th>logps / rejected</th>\n",
              "      <th>logits / chosen</th>\n",
              "      <th>logits / rejected</th>\n",
              "      <th>eval_logits / chosen</th>\n",
              "      <th>eval_logits / rejected</th>\n",
              "      <th>nll_loss</th>\n",
              "      <th>aux_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.694400</td>\n",
              "      <td>-0.003573</td>\n",
              "      <td>-0.001231</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>-0.002342</td>\n",
              "      <td>-428.614014</td>\n",
              "      <td>-380.381042</td>\n",
              "      <td>0.258835</td>\n",
              "      <td>0.095361</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.667400</td>\n",
              "      <td>-0.061980</td>\n",
              "      <td>-0.140370</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.078389</td>\n",
              "      <td>-381.055359</td>\n",
              "      <td>-366.259186</td>\n",
              "      <td>0.604749</td>\n",
              "      <td>-0.299332</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.860000</td>\n",
              "      <td>-0.229857</td>\n",
              "      <td>-0.212443</td>\n",
              "      <td>0.575000</td>\n",
              "      <td>-0.017414</td>\n",
              "      <td>-530.129639</td>\n",
              "      <td>-390.653625</td>\n",
              "      <td>0.505023</td>\n",
              "      <td>0.284466</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.708600</td>\n",
              "      <td>0.904515</td>\n",
              "      <td>0.357892</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.546623</td>\n",
              "      <td>-220.629593</td>\n",
              "      <td>-275.541687</td>\n",
              "      <td>0.628057</td>\n",
              "      <td>-0.036014</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.449000</td>\n",
              "      <td>1.230030</td>\n",
              "      <td>-0.078535</td>\n",
              "      <td>0.725000</td>\n",
              "      <td>1.308565</td>\n",
              "      <td>-292.062286</td>\n",
              "      <td>-302.435730</td>\n",
              "      <td>0.706830</td>\n",
              "      <td>-0.108548</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.589500</td>\n",
              "      <td>0.220378</td>\n",
              "      <td>0.566756</td>\n",
              "      <td>0.525000</td>\n",
              "      <td>-0.346377</td>\n",
              "      <td>-334.206543</td>\n",
              "      <td>-320.467438</td>\n",
              "      <td>0.319465</td>\n",
              "      <td>0.199990</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='76' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  76/1200 13:24 < 3:23:39, 0.09 it/s, Epoch 0.19/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>rewards / chosen</th>\n",
              "      <th>rewards / rejected</th>\n",
              "      <th>rewards / accuracies</th>\n",
              "      <th>rewards / margins</th>\n",
              "      <th>logps / chosen</th>\n",
              "      <th>logps / rejected</th>\n",
              "      <th>logits / chosen</th>\n",
              "      <th>logits / rejected</th>\n",
              "      <th>eval_logits / chosen</th>\n",
              "      <th>eval_logits / rejected</th>\n",
              "      <th>nll_loss</th>\n",
              "      <th>aux_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.694400</td>\n",
              "      <td>-0.003573</td>\n",
              "      <td>-0.001231</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>-0.002342</td>\n",
              "      <td>-428.614014</td>\n",
              "      <td>-380.381042</td>\n",
              "      <td>0.258835</td>\n",
              "      <td>0.095361</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.667400</td>\n",
              "      <td>-0.061980</td>\n",
              "      <td>-0.140370</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.078389</td>\n",
              "      <td>-381.055359</td>\n",
              "      <td>-366.259186</td>\n",
              "      <td>0.604749</td>\n",
              "      <td>-0.299332</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.860000</td>\n",
              "      <td>-0.229857</td>\n",
              "      <td>-0.212443</td>\n",
              "      <td>0.575000</td>\n",
              "      <td>-0.017414</td>\n",
              "      <td>-530.129639</td>\n",
              "      <td>-390.653625</td>\n",
              "      <td>0.505023</td>\n",
              "      <td>0.284466</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.708600</td>\n",
              "      <td>0.904515</td>\n",
              "      <td>0.357892</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.546623</td>\n",
              "      <td>-220.629593</td>\n",
              "      <td>-275.541687</td>\n",
              "      <td>0.628057</td>\n",
              "      <td>-0.036014</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.449000</td>\n",
              "      <td>1.230030</td>\n",
              "      <td>-0.078535</td>\n",
              "      <td>0.725000</td>\n",
              "      <td>1.308565</td>\n",
              "      <td>-292.062286</td>\n",
              "      <td>-302.435730</td>\n",
              "      <td>0.706830</td>\n",
              "      <td>-0.108548</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.589500</td>\n",
              "      <td>0.220378</td>\n",
              "      <td>0.566756</td>\n",
              "      <td>0.525000</td>\n",
              "      <td>-0.346377</td>\n",
              "      <td>-334.206543</td>\n",
              "      <td>-320.467438</td>\n",
              "      <td>0.319465</td>\n",
              "      <td>0.199990</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.829900</td>\n",
              "      <td>1.707637</td>\n",
              "      <td>2.173043</td>\n",
              "      <td>0.525000</td>\n",
              "      <td>-0.465406</td>\n",
              "      <td>-502.605621</td>\n",
              "      <td>-353.458099</td>\n",
              "      <td>0.577748</td>\n",
              "      <td>0.398341</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-98a88527ebc3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mdpo_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2241\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2242\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36m_unsloth_training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/UnslothDPOTrainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   1678\u001b[0m         )\n\u001b[1;32m   1679\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1680\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_loss_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m         \u001b[0;31m# Make sure to move the loss to the device the original accumulating loss is at back in the `Trainer` class:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/UnslothDPOTrainer.py\u001b[0m in \u001b[0;36mget_batch_loss_metrics\u001b[0;34m(self, model, batch, train_eval)\u001b[0m\n\u001b[1;32m   1620\u001b[0m             \u001b[0mref_rejected_logps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ref_rejected_logps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1621\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1622\u001b[0;31m             \u001b[0mref_chosen_logps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_rejected_logps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_ref_log_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m         losses, chosen_rewards, rejected_rewards = self.dpo_loss(\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/UnslothDPOTrainer.py\u001b[0m in \u001b[0;36mcompute_ref_log_probs\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m   1166\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnull_ref_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m                     \u001b[0mref_model_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenated_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m                 \u001b[0mref_model_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenated_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/UnslothDPOTrainer.py\u001b[0m in \u001b[0;36mconcatenated_forward\u001b[0;34m(self, model, batch)\u001b[0m\n\u001b[1;32m   1515\u001b[0m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1517\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1518\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m             )\n\u001b[1;32m    744\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mPeftModelForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m ):\n\u001b[0;32m-> 1200\u001b[0;31m     return self.base_model(\n\u001b[0m\u001b[1;32m   1201\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m         \u001b[0mcausal_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m_CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_no_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m             outputs = self.model(\n\u001b[0m\u001b[1;32m   1044\u001b[0m                 \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0mcausal_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mLlamaModel_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m             layer_outputs = torch.utils.checkpoint.checkpoint(\n\u001b[0m\u001b[1;32m    859\u001b[0m                 \u001b[0mcreate_custom_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m             )\n\u001b[1;32m    744\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/gradient_checkpointing.py\u001b[0m in \u001b[0;36munsloth_checkpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;34m\"use_reentrant=False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m             )\n\u001b[0;32m--> 747\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mUnslothCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         gen = _checkpoint_without_reentrant_generator(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/gradient_checkpointing.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_gpu_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMAIN_STREAM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEXTRA_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mcustom_forward\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mcreate_custom_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mcustom_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mLlamaDecoderLayer_fast_forward\u001b[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, position_embeddings, *args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfast_rms_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mMatMul4Bit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# 1. Dequantize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;31m# 2. MatmulnN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdequantize_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;31m# 3. Save state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Patch DPO Trainer first\n",
        "from unsloth import PatchDPOTrainer\n",
        "PatchDPOTrainer()\n",
        "\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# Configure DPO training\n",
        "dpo_config = DPOConfig(\n",
        "    beta=0.1,\n",
        "    learning_rate=5e-5,\n",
        "    max_length=1024,\n",
        "    max_prompt_length=512,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_strategy=\"no\",\n",
        "    bf16=is_bfloat16_supported(),\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    ref_model=None,  # Optional: you can pass a frozen reference model\n",
        "    args=dpo_config,\n",
        "    train_dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Train\n",
        "dpo_trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gay0MUe_1YnU"
      },
      "outputs": [],
      "source": [
        "from trl import DPOTrainer, DPOConfig\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v1h1ajk10Q3"
      },
      "outputs": [],
      "source": [
        "dpo_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Cq5F53hrxBY"
      },
      "source": [
        "### Save the Model (2.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "7w-BG1svrxBZ",
        "outputId": "87557b40-ae01-4dc0-f2d0-cc9c26fee5d8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-102f60c75235>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save model and tokenizer to local directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dpo-lora-model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dpo-lora-model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# Save model and tokenizer to local directory\n",
        "model.save_pretrained(\"dpo-lora-model\")\n",
        "tokenizer.save_pretrained(\"dpo-lora-model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JST2NuBT1wLc"
      },
      "source": [
        "### Inference (2.5 points)\n",
        "- Enable faster inference with Unsloth.\n",
        "- Generate output for two randomly selected samples from the `orpo-dpo-mix-40k` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gp-_ME661Yix",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "ad779cfe-16e8-4285-c6cf-e37085cb2e3d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-fc134b443f5b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Enable FastLanguageModel inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Optimizes model for fast generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Enable FastLanguageModel inference\n",
        "model = FastLanguageModel.from_pretrained(model, dtype=None, load_in_4bit=True)\n",
        "model = FastLanguageModel.for_inference(model)  # Optimizes model for fast generation\n",
        "\n",
        "# Sample 2 prompts\n",
        "sample_prompts = random.sample(dataset[\"prompt\"], 2)\n",
        "dpo_responses = []\n",
        "\n",
        "for prompt in sample_prompts:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, max_new_tokens=100)\n",
        "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    dpo_responses.append(decoded)\n",
        "\n",
        "# View responses\n",
        "for i, response in enumerate(dpo_responses):\n",
        "    print(f\"\\nPrompt {i+1}:\\n{sample_prompts[i]}\")\n",
        "    print(f\"Response:\\n{response}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23IyRIAW58rW"
      },
      "source": [
        "### Evaluate with Reward Model (2.5 points)\n",
        "\n",
        "- Estimate the rewards of generated responses.\n",
        "\n",
        "    **Note:** Consider memory management in this section. If you encounter an **Out of Memory** issue, you should save the responses after making inferences from the model, free up GPU memory, and then load the Reward Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaCbVjxlyRLm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# OPTIONAL: Free memory before loading reward model\n",
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Load reward model\n",
        "reward_model_id = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n",
        "reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_id)\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_id).to(\"cuda\")\n",
        "reward_model.eval()\n",
        "\n",
        "def score_response_with_reward_model(prompt, response):\n",
        "    input_text = f\"{prompt}\\n\\n{response}\"\n",
        "    inputs = reward_tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = reward_model(**inputs)\n",
        "    return outputs.logits.item()\n",
        "\n",
        "# Evaluate the two previously generated responses\n",
        "reward_scores = []\n",
        "for prompt, response in zip(sample_prompts, dpo_responses):\n",
        "    score = score_response_with_reward_model(prompt, response)\n",
        "    reward_scores.append(score)\n",
        "    print(f\"\\nPrompt:\\n{prompt}\")\n",
        "    print(f\"Response:\\n{response}\")\n",
        "    print(f\"Reward Score: {score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeYK6BAl4Z0Z"
      },
      "source": [
        "## ORPO (20 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bybMkHt9rxBa"
      },
      "source": [
        "<img src=\"https://arxiv.org/html/2403.07691v1/x2.png\" style=\"background-color:white; padding:10px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN8X5CH04k5Z"
      },
      "source": [
        "### Question 8 (5 points):\n",
        "\n",
        "Traditional preference alignment methods, such as Reinforcement Learning with Human Feedback (RLHF) and Direct Preference Optimization (DPO), often rely on a separate reference model to guide the optimization process. [ORPO](https://arxiv.org/abs/2403.07691), however, eliminates this dependency.\n",
        "\n",
        "**a.** Explain why removing the reference model simplifies preference optimization in language models.\n",
        "\n",
        "**b.** Discuss the potential advantages and disadvantages of this approach compared to RLHF and DPO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz3Q1AcXrxBa"
      },
      "source": [
        "**(a) Why Removing the Reference Model Simplifies Preference Optimization**\n",
        "\n",
        "In methods such as **Reinforcement Learning with Human Feedback (RLHF)** and **Direct Preference Optimization (DPO)**, a **reference model** (or ‚Äúbaseline‚Äù policy) often serves as a stability anchor, discouraging the newly updated policy from drifting too far from previously learned behaviors. **ORPO (Offline Relative Policy Optimization)** omits this reference model entirely. As a result, the **policy** being trained is directly optimized based on **pairwise preferences** or offline data without explicitly comparing its outputs to a fixed baseline. This design leads to simplifications:\n",
        "\n",
        "1. **Fewer Components to Manage**  \n",
        "   - Eliminating the reference model means there is no need to maintain, load, or synchronize a separate checkpoint. This reduces engineering overhead and memory usage.\n",
        "\n",
        "2. **Direct Policy Updates**  \n",
        "   - Instead of bounding the new policy by a KL divergence to the old policy, ORPO directly optimizes the model‚Äôs parameters with respect to the preference data. This can simplify the objective function, removing extra hyperparameters or clipping terms needed to stabilize the comparison between two policies.\n",
        "\n",
        "3. **Less Dependency on Carefully Chosen Baselines**  \n",
        "   - In RLHF or DPO, picking or updating the reference model incorrectly can lead to suboptimal or unstable results. ORPO sidesteps this complexity by **not** anchoring updates to a baseline, relying instead on offline preference signals alone.\n",
        "\n",
        "---\n",
        "\n",
        "**(b) Advantages and Disadvantages Compared to RLHF and DPO**\n",
        "\n",
        "1. **Advantages**\n",
        "\n",
        "   - **Reduced Complexity**: Without maintaining a reference model, the training pipeline is simpler‚Äîfewer hyperparameters (like KL coefficients) and fewer moving parts (no separate baseline or ‚Äúold policy‚Äù).\n",
        "   - **Potentially Faster Updates**: ORPO can more aggressively optimize for the preference data since it does not strictly constrain policy changes against a baseline. This may yield quicker adaptation if the offline data is representative.\n",
        "   - **Lower Resource Usage**: Not storing or continually synchronizing a reference model can save computational resources, which is especially valuable for large language models.\n",
        "\n",
        "2. **Disadvantages**\n",
        "\n",
        "   - **Risk of Overfitting**: Without a reference model acting as a stability anchor, the policy might overfit to the available preference data, losing generalization or reverting to degenerate text generation (e.g., repeating the same high-scoring patterns).\n",
        "   - **Less ‚ÄúSafety Net‚Äù**: Methods like RLHF typically rely on a KL term or clipped objective to prevent catastrophic changes. ORPO, lacking a reference model, must rely on alternative mechanisms (e.g., curated offline data, other forms of regularization) to ensure stable, gradual updates.\n",
        "   - **Sensitivity to Data Quality**: Because the policy is updated purely from offline preference data without a baseline, any biases or inaccuracies in the dataset may lead the model astray. In RLHF or DPO, the reference model can partially mitigate poor data by providing a fallback distribution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhwpNIbspeDA"
      },
      "source": [
        "### Train the model (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
        "from trl import DPOTrainer, DPOConfig\n"
      ],
      "metadata": {
        "id": "VRisSdDBiI-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UaN0_h5rxBa"
      },
      "source": [
        "- Follow the steps as in the DPO section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpEOkLMYrxBa"
      },
      "outputs": [],
      "source": [
        "class DPODataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # Each example has a prompt, a chosen response, and a rejected response\n",
        "        self.data = [\n",
        "            {\n",
        "                \"prompt\": \"Explain why the sky is blue.\",\n",
        "                \"chosen\": \"The sky appears blue due to the scattering of sunlight by the atmosphere...\",\n",
        "                \"rejected\": \"Because I said so!\"\n",
        "            },\n",
        "            {\n",
        "                \"prompt\": \"What is 2 + 2?\",\n",
        "                \"chosen\": \"2 + 2 is 4.\",\n",
        "                \"rejected\": \"2 + 2 is 22, obviously.\"\n",
        "            }\n",
        "            # Add more data as needed...\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "train_dataset = DPODataset()\n",
        "val_dataset   = DPODataset()  # In real use, you'd have a separate validation set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7Y4YQI7rxBa"
      },
      "source": [
        "- Set up ORPOTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYj2-3WNY2MY"
      },
      "outputs": [],
      "source": [
        "model_name = \"gpt2\"  # or \"facebook/opt-350m\", \"meta-llama/Llama-2-7b-hf\", etc.\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a real pad token\n",
        "\n",
        "policy_model    = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "reference_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "reward_model    = AutoModelForCausalLM.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dpo_config = DPOConfig(\n",
        "    # If you have separate model paths:\n",
        "    model_name_or_path=model_name,\n",
        "    reference_model_name_or_path=model_name,\n",
        "    reward_model_name_or_path=model_name,\n",
        "\n",
        "    # Hyperparameters\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=1,\n",
        "    # The rest can be tuned or use defaults\n",
        ")\n"
      ],
      "metadata": {
        "id": "5cOGApmZiYug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./dpo-output\",\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"no\",  # or \"steps\" if you want to evaluate\n",
        "    save_strategy=\"no\",        # or \"steps\"\n",
        "    max_steps=10,              # just for quick demo\n",
        "    # etc.\n",
        ")\n"
      ],
      "metadata": {
        "id": "BlNgE_NEiZ1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = DPOTrainer(\n",
        "    config=dpo_config,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "\n",
        "    model=policy_model,\n",
        "    ref_model=reference_model,\n",
        "    reward_model=reward_model,\n",
        "\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n"
      ],
      "metadata": {
        "id": "8HheXLeGibbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfUxFREMYmq8"
      },
      "outputs": [],
      "source": [
        "orpo_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMHZ06JfrxBa"
      },
      "source": [
        "- Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKlhjNMlrxBb"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"./dpo-finetuned-model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHqExhtpp-BE"
      },
      "source": [
        "### Inference (2.5 points)\n",
        "- Make an inference on two randomly selected samples (similar to the DPO section)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig6ASkWfp6GU"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the trained policy model from disk\n",
        "finetuned_model = AutoModelForCausalLM.from_pretrained(\"./dpo-finetuned-model\")\n",
        "finetuned_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=finetuned_model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        ")\n",
        "\n",
        "sample_prompts = [\n",
        "    \"Explain why the sky is blue.\",\n",
        "    \"What is 2 + 2?\"\n",
        "]\n",
        "dpo_responses = []\n",
        "\n",
        "for prompt in sample_prompts:\n",
        "    output = finetuned_pipe(\n",
        "        prompt,\n",
        "        max_new_tokens=50,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    response = output[0][\"generated_text\"]\n",
        "    dpo_responses.append(response)\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qZJQ-DdrxBb"
      },
      "source": [
        "### Evaluate with Reward Model (5 points)\n",
        "\n",
        "- Estimate the rewards of generated responses.\n",
        "- Compare DPO and ORPO results.\n",
        "\n",
        "    **Note:** Consider memory management in this section. If you encounter an **Out of Memory** issue, you should save the responses after making inferences from the model, free up GPU memory, and then load the Reward Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwWuAXykrxBb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# If needed, reload your specialized reward model\n",
        "# reward_model = AutoModelForCausalLM.from_pretrained(\"my-specialized-reward-model\")\n",
        "\n",
        "rewards = []\n",
        "for prompt, response in zip(sample_prompts, dpo_responses):\n",
        "    # Concatenate prompt & response in a way your reward model expects\n",
        "    # Some reward models might want: [PROMPT] [SEP] [RESPONSE]\n",
        "    text_to_evaluate = prompt + \"\\n\" + response\n",
        "\n",
        "    inputs = tokenizer(text_to_evaluate, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # For a real reward model, you might do something like:\n",
        "        # reward_score = reward_model(**inputs).logits.squeeze()\n",
        "        # Then interpret that as a scalar reward.\n",
        "        outputs = reward_model(**inputs)\n",
        "        # This is purely illustrative:\n",
        "        reward_score = outputs.logits.mean()  # A stand-in for \"reward\"\n",
        "\n",
        "    rewards.append(reward_score.item())\n",
        "\n",
        "avg_reward = sum(rewards) / len(rewards)\n",
        "print(\"Rewards:\", rewards)\n",
        "print(\"Average Reward:\", avg_reward)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgHZNB0dqrJ6"
      },
      "source": [
        "### Question 9 (2.5 points):\n",
        "\n",
        "Compare DPO and ORPO in terms of execution time and VRAM used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCPME8yP9Sh5"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZYIjV_hrxBb"
      },
      "source": [
        "# **Optional Section** (10 points):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS-h-7_irxBb"
      },
      "source": [
        "### **Evaluating the Impact of Alignment on ICL**\n",
        "\n",
        "In this section, you will re-evaluate the **in-context learning (ICL) performance** after aligning the model with **DPO** and **ORPO**. The goal is to analyze how alignment affects the model‚Äôs ability to follow different prompting strategies.\n",
        "\n",
        "1. **Use the same evaluation setup** from the [Prompt Engineering](#prompt-engineering) section.\n",
        "2. **Re-run the model** on the same [GSM8K](#gsm8k_benchmark) tasks.\n",
        "3. **Document your observations** in a table:\n",
        "\n",
        "| Model Version  | Accuracy (%) | Common Errors |\n",
        "|---------------|------------|--------------|\n",
        "| Baseline       | XX%        | \\<list errors> |\n",
        "| Post-DPO      | XX%        | \\<list errors> |\n",
        "| Post-ORPO      | XX%        | \\<list errors> |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCHdfq33rxBb"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTs6yMssrxBb"
      },
      "source": [
        "### **Discussion:**\n",
        "- Does preference alignment improve or degrade raw performance?\n",
        "- Does the model respond differently to variations in prompts?\n",
        "- How does alignment impact the model's **reasoning consistency** in prompts like CoT?\n",
        "\n",
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDsXcuYW99qc"
      },
      "source": [
        "# AI Disclosure\n",
        "\n",
        "*   Did you use any AI assistance to complete this homework? If so, please also specify what AI you used.\n",
        "    * *Chat GPT*\n",
        "\n",
        "\n",
        "---\n",
        "*(only complete the below questions if you answered yes above)*\n",
        "\n",
        "*   If you used a large language model to assist you, please paste prompts that you used below. Add a separate bullet for each prompt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MhmwKXArxBd"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7c453e750ee44c90a9b9dc8be5d5fb43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4e2819a336849018ed46dc79734f385",
              "IPY_MODEL_536fa11665f6428b97f620c559961367",
              "IPY_MODEL_36ee5d6a1d7f4c97a36938a2ba223186"
            ],
            "layout": "IPY_MODEL_45558c900a4e464b8d87f3a65bb92246"
          }
        },
        "c4e2819a336849018ed46dc79734f385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ca4cbcfdae7427dae77a04995d09cf9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a9d1a63481a84d0e8a6a3cbb0b90b96b",
            "value": "README.md:‚Äá100%"
          }
        },
        "536fa11665f6428b97f620c559961367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6eada20d71d42c5a190c2732736ea89",
            "max": 7940,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_190f929bac144b3c8f9ce57f8b2aeaf9",
            "value": 7940
          }
        },
        "36ee5d6a1d7f4c97a36938a2ba223186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6e377f98c7f4524a1e7aaffc15f35c9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1f12479477474537bb4c241fce06d6e0",
            "value": "‚Äá7.94k/7.94k‚Äá[00:00&lt;00:00,‚Äá464kB/s]"
          }
        },
        "45558c900a4e464b8d87f3a65bb92246": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ca4cbcfdae7427dae77a04995d09cf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9d1a63481a84d0e8a6a3cbb0b90b96b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6eada20d71d42c5a190c2732736ea89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "190f929bac144b3c8f9ce57f8b2aeaf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6e377f98c7f4524a1e7aaffc15f35c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f12479477474537bb4c241fce06d6e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e973bc57470b41bda53e6697d2632669": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44aa7d9c8ea445139ab469695f80168a",
              "IPY_MODEL_ce250c8ba31f4bfda00f0d3ded9ecbf0",
              "IPY_MODEL_6b472cb679704c378cdf41ce26b99393"
            ],
            "layout": "IPY_MODEL_966ee1eab5174e4e960c58f94fbce30b"
          }
        },
        "44aa7d9c8ea445139ab469695f80168a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f46bbd3fe61340fc87a2716c3a18eaaf",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_459e480df3e342049d0c52f878bdb7c9",
            "value": "train-00000-of-00001.parquet:‚Äá100%"
          }
        },
        "ce250c8ba31f4bfda00f0d3ded9ecbf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4f59467ce624125b10acda2552db860",
            "max": 2306545,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f918727de5f4391a734dbe3f52ec1cc",
            "value": 2306545
          }
        },
        "6b472cb679704c378cdf41ce26b99393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b72b7986229941ca82876c6bdf31f3ae",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4bb8b3924d474acfb40c8b25add33cbb",
            "value": "‚Äá2.31M/2.31M‚Äá[00:00&lt;00:00,‚Äá36.4MB/s]"
          }
        },
        "966ee1eab5174e4e960c58f94fbce30b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f46bbd3fe61340fc87a2716c3a18eaaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "459e480df3e342049d0c52f878bdb7c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4f59467ce624125b10acda2552db860": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f918727de5f4391a734dbe3f52ec1cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b72b7986229941ca82876c6bdf31f3ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bb8b3924d474acfb40c8b25add33cbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "001290d3ed254e16b8e75eb3ecb2b901": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40230e550b2c481bbb70f505f723b6bd",
              "IPY_MODEL_96ddc0f32e3c4c929ccf489a2a1a11b4",
              "IPY_MODEL_3382c376cd064efe9e249eec782b944c"
            ],
            "layout": "IPY_MODEL_0aa0f249959b4b4ea9f01bd5a6c51d17"
          }
        },
        "40230e550b2c481bbb70f505f723b6bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d99a3b72b1746bf9b77e53ededcf34e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8c331e31992c40c79994b7db647e3b8a",
            "value": "test-00000-of-00001.parquet:‚Äá100%"
          }
        },
        "96ddc0f32e3c4c929ccf489a2a1a11b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71b64e6e0f3c4f3b8fcdeb2615654ee0",
            "max": 419088,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af001559668042f3a82d2965fb5cd3d2",
            "value": 419088
          }
        },
        "3382c376cd064efe9e249eec782b944c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d8aa9820d7b4c44960f059d7f261b93",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0add3a61bcd54ad29c883a12f836703d",
            "value": "‚Äá419k/419k‚Äá[00:00&lt;00:00,‚Äá36.0MB/s]"
          }
        },
        "0aa0f249959b4b4ea9f01bd5a6c51d17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d99a3b72b1746bf9b77e53ededcf34e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c331e31992c40c79994b7db647e3b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71b64e6e0f3c4f3b8fcdeb2615654ee0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af001559668042f3a82d2965fb5cd3d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d8aa9820d7b4c44960f059d7f261b93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0add3a61bcd54ad29c883a12f836703d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4b258bcb78844a085327985e96b44dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c96680953e814b2d8a63818c3068a32c",
              "IPY_MODEL_d942c03267dd45cfabb6a17f0ac7180c",
              "IPY_MODEL_7e610bc9e1684287926b96690db7a44a"
            ],
            "layout": "IPY_MODEL_2c426ca993114cf8950bfa8f48d6aeda"
          }
        },
        "c96680953e814b2d8a63818c3068a32c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e63a3ca70774489b50b12218effa91b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_221bdd5cd1714f289ab71482643ca0a2",
            "value": "Generating‚Äátrain‚Äásplit:‚Äá100%"
          }
        },
        "d942c03267dd45cfabb6a17f0ac7180c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3e98c6298d1413581602a1881eafd16",
            "max": 7473,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5005a77a69e4256a78a17a4a98a790f",
            "value": 7473
          }
        },
        "7e610bc9e1684287926b96690db7a44a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01c721b2fe8d4674a260c44b4f3b1945",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_71fba08264684b2da3b050c9ef3b2b02",
            "value": "‚Äá7473/7473‚Äá[00:00&lt;00:00,‚Äá9692.32‚Äáexamples/s]"
          }
        },
        "2c426ca993114cf8950bfa8f48d6aeda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e63a3ca70774489b50b12218effa91b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "221bdd5cd1714f289ab71482643ca0a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3e98c6298d1413581602a1881eafd16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5005a77a69e4256a78a17a4a98a790f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "01c721b2fe8d4674a260c44b4f3b1945": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71fba08264684b2da3b050c9ef3b2b02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34c5cfd94211424d92b11c98fdacc343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7b4b520dd4140d6b8d5a3a39c6c95a8",
              "IPY_MODEL_cfec4744815541d7b6653c3ef2bbc360",
              "IPY_MODEL_b92cb033185a4c42acc2f309e4d7a05b"
            ],
            "layout": "IPY_MODEL_6e43078785804e57ad8e4e7aabbe18ca"
          }
        },
        "c7b4b520dd4140d6b8d5a3a39c6c95a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfde114a88254c31b6df776a71e7f48c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e2452d43bba640fb83a8e4da3651de39",
            "value": "Generating‚Äátest‚Äásplit:‚Äá100%"
          }
        },
        "cfec4744815541d7b6653c3ef2bbc360": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf97ffb1a48f4388862ccb2373e46179",
            "max": 1319,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2da73a0393764de99cb88bf22b5b4522",
            "value": 1319
          }
        },
        "b92cb033185a4c42acc2f309e4d7a05b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb5dc93cb2644b809093c14a7876747c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9afff7605be2492fb5bd649a00c9d3cc",
            "value": "‚Äá1319/1319‚Äá[00:00&lt;00:00,‚Äá58479.60‚Äáexamples/s]"
          }
        },
        "6e43078785804e57ad8e4e7aabbe18ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfde114a88254c31b6df776a71e7f48c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2452d43bba640fb83a8e4da3651de39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf97ffb1a48f4388862ccb2373e46179": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2da73a0393764de99cb88bf22b5b4522": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb5dc93cb2644b809093c14a7876747c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9afff7605be2492fb5bd649a00c9d3cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}