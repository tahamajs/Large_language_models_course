{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLJYAhNjjFUF"
      },
      "source": [
        "# **CA 2, LLMs Spring 2025**\n",
        "\n",
        "- **Name:**\n",
        "- **Student ID:**\n",
        "\n",
        "---\n",
        "#### Your submission should be named using the following format: `CA2_LASTNAME_STUDENTID.ipynb`.\n",
        "\n",
        "---\n",
        "\n",
        "##### *How to do this problem set:*\n",
        "\n",
        "- Some questions require writing Python code and computing results, and the rest of them have written answers. For coding problems, you will have to fill out all code blocks that say `YOUR CODE HERE`.\n",
        "\n",
        "- For text-based answers, you should replace the text that says ```Your Answer Here``` with your actual answer.\n",
        "\n",
        "- There is no penalty for using AI assistance on this homework as long as you fully disclose it in the final cell of this notebook (this includes storing any prompts that you feed to large language models). That said, anyone caught using AI assistance without proper disclosure will receive a zero on the assignment (we have several automatic tools to detect such cases). We're literally allowing you to use it with no limitations, so there is no reason to lie!\n",
        "\n",
        "---\n",
        "\n",
        "##### *Academic honesty*\n",
        "\n",
        "- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your notebook. If you turn in correct answers on your notebook without code that actually generates those answers, we will consider this a serious case of cheating.\n",
        "\n",
        "- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n",
        "\n",
        "---\n",
        "\n",
        "If you have any further questions or concerns, contact the TAs via email: m.salmani78@ut.ac.ir / mehrabi.m@ut.ac.ir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAV1HzrhLd7c"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install datasets Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfDbur2dfcjy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from Levenshtein import ratio\n",
        "from collections import defaultdict\n",
        "from datasets import load_dataset\n",
        "from trl import ORPOConfig, ORPOTrainer\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNY4jz4Lkwtb"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login --token {YOUR_HF_TOKEN}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSKRoHpW7Qd-"
      },
      "outputs": [],
      "source": [
        "class CONFIG:\n",
        "    seed = 42\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
        "    reward_model_name = \"nicolinho/QRM-Llama3.1-8B-v2\"\n",
        "    benchmark_name = \"openai/gsm8k\"\n",
        "    dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\n",
        "\n",
        "    train_data_size = 1600\n",
        "    benchmark_subset_size = 50\n",
        "    max_seq_length = 2048\n",
        "    train_batch_size = 2\n",
        "    gradient_accumulation_steps = 4\n",
        "    epochs = 1\n",
        "\n",
        "    # LoRA Configs\n",
        "    lora_rank = 64,\n",
        "    lora_alpha = 64,\n",
        "    use_gradient_checkpointing = \"unsloth\"\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "\n",
        "    dpo_output_dir = \"llama-3.2-3b-dpo-checkpoint\"\n",
        "    orpo_output_dir = \"llama-3.2-3b-orpo-checkpoint\"\n",
        "\n",
        "device = CONFIG.device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRuSzJeqJYsJ"
      },
      "source": [
        "### Introductions to unsloth\n",
        "\n",
        "Modern large language models (LLMs) require significant computational resources for fine-tuning and inference. The `unsloth` library is designed to optimize these processes by making training up to 30Ã— faster and reducing memory usage by 60%, enabling more efficient model adaptation on consumer-grade GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgSHp25MMnhL"
      },
      "source": [
        "---\n",
        "\n",
        "**Learn More:**\n",
        "\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Eb4IM4iLbW9"
      },
      "source": [
        "### Install and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_roQDImZJE5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "!pip install datasets\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "    !pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neZPEJBplk3w"
      },
      "outputs": [],
      "source": [
        "import unsloth\n",
        "print(unsloth.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiWagCG4pjww"
      },
      "source": [
        "# In-context Learning (30 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fjyU22wCiOd"
      },
      "source": [
        "### Question 1 (5 points):\n",
        "\n",
        "**a)** What is In-Context Learning (ICL), and how does it differ from fine-tuning? What are its limitations compared to fine-tuning?\n",
        "\n",
        "**b)** Explain what [Chain-of-Thought (CoT)](https://arxiv.org/abs/2201.11903) prompting is and how it works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laSJdBpgqE-M"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npgTNJf7lEAq"
      },
      "source": [
        "### Load Model & Tokenizer (2.5 points)\n",
        "\n",
        "- Load `Llama-3.2-3B-Instruct-bnb-4bit` model using `unsloth` for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2JUGaj4lB3w"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "def load_model_and_tokenizer(model_id, max_seq_length):\n",
        "    print(\"Loading model and tokenizer using unsloth...\")\n",
        "    # WRITE YOUR CODE HERE\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bDLhdusm0m3"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = load_model_and_tokenizer(CONFIG.model_name, CONFIG.max_seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_1wFWx7rWUo"
      },
      "source": [
        "### Load benchmark (2.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftAg-qauF8HW"
      },
      "source": [
        "1. Load the `GSM8K` benchmark dataset.\n",
        "2. Randomly select a subset of `50` samples from the dataset.\n",
        "3. Display one sample from the selected subset.\n",
        "<a id=\"gsm8k_benchmark\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jfzk-b4Dm8Hi"
      },
      "outputs": [],
      "source": [
        "def load_gsm8k_dataset():\n",
        "    \"\"\"Load the GSM8K dataset from HuggingFace.\"\"\"\n",
        "    # WRITE YOUR CODE HERE\n",
        "    return dataset\n",
        "\n",
        "def create_sample_dataset(dataset, num_samples, seed):\n",
        "    \"\"\"Create a fixed sample dataset for evaluation.\"\"\"\n",
        "    # WRITE YOUR CODE HERE\n",
        "    return sample_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL3-SUvTkpom"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_gsm8k_dataset()\n",
        "\n",
        "# Select subset\n",
        "sample_dataset = create_sample_dataset(dataset, num_samples=CONFIG.benchmark_subset_size, seed=CONFIG.seed)\n",
        "\n",
        "# Display one sample\n",
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry2nMjXwsox_"
      },
      "source": [
        "### Prompt Engineering (10 points)\n",
        "\n",
        "Implement different prompting strategies for in-context learning.\n",
        "At least four of the following methods should be implemented (including baseline):\n",
        "- Zero-shot (**Baseline**)\n",
        "- Role-play prompting [[paper](https://aclanthology.org/2024.naacl-long.228/)]\n",
        "- Zero-shot CoT [[paper](https://arxiv.org/abs/2205.11916)]\n",
        "- Few-shot CoT\n",
        "- Least-to-Most prompting [[paper](https://arxiv.org/abs/2205.10625)]\n",
        "- Generated Knowledge prompting [[paper](https://aclanthology.org/2022.acl-long.225/)]\n",
        "- Any other idea to improve performance (**Optional**)\n",
        "\n",
        "Additionally, if performance exceeds 80%, **two extra points** are awarded for every 5% improvement. You can try other methods or a combination of existing ones.\n",
        "\n",
        "<a id=\"prompt-engineering\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RgBpE5vreCN"
      },
      "outputs": [],
      "source": [
        "def create_prompts(question, examples=None):\n",
        "    \"\"\"Generate various prompt types for a given question.\"\"\"\n",
        "\n",
        "    # Zero-shot (Baseline)\n",
        "    zero_shot = (\n",
        "        f\"Problem: {question}\"\n",
        "        \"\\n\\nThe answer number is \"\n",
        "    )\n",
        "\n",
        "    # WRITE YOUR CODE HERE\n",
        "\n",
        "    return {\n",
        "        \"Baseline\": zero_shot,\n",
        "        # \"Role-Prompting\": role_prompting,\n",
        "        # \"Zero-shot CoT\": zero_shot_cot,\n",
        "        # \"Few-shot CoT\": few_shot_cot,\n",
        "        # \"Least-to-Most\": least_to_most_prompting,\n",
        "        # \"Generated Knowledge\": generated_knowledge_prompting,\n",
        "        # **Optional additional strategies**\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbYUEqf_svlx"
      },
      "source": [
        "### Evaluate Prompting Strategies (10 points)\n",
        "\n",
        "1. Implement an evaluation function to assess different prompts.\n",
        "2. Compare the accuracy of various prompting methods.\n",
        "3. Visualize results and show some sample responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arWJGLhnZRCk"
      },
      "outputs": [],
      "source": [
        "def extract_answer(text):\n",
        "    \"\"\"Extract the final numerical answer from the model's output\"\"\"\n",
        "    # WRITE YOUR CODE HERE\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x6PthPerlWN"
      },
      "outputs": [],
      "source": [
        "def evaluate_prompts(model, tokenizer, sample_dataset, seed=42):\n",
        "    \"\"\"Evaluate all prompt variations on the provided sample dataset.\"\"\"\n",
        "    # WRITE YOUR CODE HERE\n",
        "    return accuracy, all_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dP9_kZlNtBKr"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy by prompting method:\")\n",
        "accuracy, all_samples = evaluate_prompts(model, tokenizer, sample_dataset)\n",
        "\n",
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLpAor8Os00J"
      },
      "outputs": [],
      "source": [
        "def visualize_results(model_name, accuracies):\n",
        "    \"\"\"Create a bar chart of prompt method accuracies.\"\"\"\n",
        "    # WRITE YOUR CODE HERE\n",
        "    return plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP5-UCT_ttKV"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZkNyO8ZklsM"
      },
      "source": [
        "# Human Preference Alignment (80 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z66fIMmWq-7o"
      },
      "source": [
        "## RLHF Flow\n",
        "\n",
        "<img src=\"https://huyenchip.com/assets/pics/rlhf/6-sft-rlhf.png\" width=\"80%\">\n",
        "\n",
        "With the rise of **ChatGPT**, **Reinforcement Learning from Human Feedback (RLHF)** has gained significant attention in both academic and industrial language modeling communities.\n",
        "\n",
        "The approach dates back to **OpenAIâ€™s 2019 paper**:  \n",
        "[Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593).  \n",
        "\n",
        "A year later, OpenAI demonstrated RLHFâ€™s effectiveness in **natural language generation**:  \n",
        "[Learning to Summarize from Human Feedback](https://arxiv.org/abs/2009.01325).  \n",
        "\n",
        "This research showed that fine-tuning alone leads to **suboptimal human-aligned performance**. RLHF optimizes models using human feedback, significantly improving their output quality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNOqdRXHh5XY"
      },
      "source": [
        "## Reward Models (20 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Sog7dXylZT9"
      },
      "source": [
        "### Question 2 (5 points):\n",
        "<img width=\"50%\" alt=\"image\" src=\"https://github.com/RLHFlow/RLHFlow.github.io/blob/main/assets/BT-and-Pref-RMs.png?raw=true\">\n",
        "\n",
        "In Reinforcement Learning from Human Feedback (RLHF), the reward model is essential for aligning large language models with human preferences. A widely used method, based on the **Bradley-Terry** model, trains the reward model using the following pairwise ranking loss function for a prompt and two responses (<font color='green'><b>chosen</b></font> and <font color='red'><b>rejected</b></font>):\n",
        "\n",
        "$$\n",
        "\\text{loss}(r_{\\theta}) = -\\mathbb{E}_{(x, y_0, y_1, i) \\sim D} \\left[ \\log \\left( \\sigma \\left( r_{\\theta}(x, y_i) - r_{\\theta}(x, y_{1-i}) \\right) \\right) \\right]\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $x$ is the prompt,\n",
        "- $y_0$ and $y_1$ are two responses,\n",
        "- $i$ (0 or 1) indicates the human-preferred response,\n",
        "- $r_{\\theta}(x, y)$ is the reward modelâ€™s scalar value for the prompt $ x $ and the response $ y $,\n",
        "- $\\sigma$ is the sigmoid function.\n",
        "\n",
        "**a)** How this loss function encourages higher scores for preferred responses.\n",
        "\n",
        "**b)** Discuss one potential limitation of this approach, such as reward hacking (e.g., favoring longer responses), and suggest a general strategy to mitigate it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRAs9mMwhisg"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWmCEsDvkinK"
      },
      "source": [
        "### Question 3 (5 points):\n",
        "\n",
        "The Bradley-Terry model is widely used in RLHF to train reward models by converting pairwise human preferences into a single scalar value. However, this approach has limitations when capturing complex human values like helpfulness, honesty, and safety, which may require multiple dimensions.\n",
        "\n",
        "**a)** Why a single scalar reward might fail to capture trade-offs between objectives like helpfulness and safety, using a concrete example (e.g., a response to a user query).\n",
        "\n",
        "**b)** Describe one alternative method to the Bradley-Terry model that addresses these limitations, such as by considering multiple objectives, mitigating biases, or improving interpretability. (For inspiration, explore resources like this [repository](https://github.com/RLHFlow/RLHF-Reward-Modeling/) or this [paper](https://arxiv.org/abs/2406.12845)). How does this alternative improve upon the single-scalar approach?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX34y9qu_eby"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7WKTlsevH8I"
      },
      "source": [
        "---\n",
        "\n",
        "**Find More:**\n",
        "<br>[RewardBench LeaderBoard](https://huggingface.co/learn/deep-rl-course/en/unit0/introduction)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZkpabXSl7q4"
      },
      "source": [
        "### Inference from the Reward Model (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ied3eJK0EX0H"
      },
      "source": [
        "<div align=\"center\"><img width=\"90%\" alt=\"image\" src=\"https://github.com/Nicolinho/QRM/blob/main/assets/method_vis.png?raw=true\"></div>\n",
        "\n",
        "**Quantile Reward Models (QRM)** generates a distribution over rewards by aggregating individual distributions over attribute scores like helpfulness and harmlessness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j38krUM6xmCa"
      },
      "source": [
        "- Load the [reward model](https://huggingface.co/nicolinho/QRM-Llama3.1-8B-v2) and its tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGFpVCPIWmvD"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETBOFl7rynjz"
      },
      "source": [
        "- Generate reward scores for both responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CI06X2yJheyK"
      },
      "outputs": [],
      "source": [
        "# Prompt and responses\n",
        "sample_prompt = \"Do wooden pencils contain lead as their core?\"\n",
        "chosen_response = \"No, wooden pencils do not contain lead in their core. The term \\\"lead\\\" is a misnomer, as wooden pencils actually use graphite for their core. Graphite was historically called \\\"black lead\\\" due to its appearance, leading to the common misconception that pencils contain lead.\"\n",
        "rejected_response = \"Yes, wooden pencils typically contain a core made of graphite and clay, which is commonly referred to as \\\"lead\\\" despite not being made of actual lead.\"\n",
        "\n",
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0g5d58WrSaI"
      },
      "source": [
        "- Visualize the results:\n",
        "\n",
        "    + Create a bar chart comparing the reward scores of the chosen vs. the rejected response for each attribute.\n",
        "    + Overlay a line chart representing the gating output coefficients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkkyvEBKrQPZ"
      },
      "outputs": [],
      "source": [
        "# The attributes of the 5 reward objectives\n",
        "attributes = ['helpfulness','correctness','coherence', 'complexity','verbosity']\n",
        "\n",
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka51QlNlkCgo"
      },
      "source": [
        "## PPO (15 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVTtkH-XkG08"
      },
      "source": [
        "### Question 4 (5 points):\n",
        "**a)** Describe the Proximal Policy Optimization (PPO) algorithm and explain its role in the Reinforcement Learning from Human Feedback (RLHF) framework.\n",
        "\n",
        "**b)** Specifically, is PPO an on-policy or off-policy algorithm, and why is this characteristic important for its application in RLHF?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr50YXvFkp5D"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45WO6uGkkuRA"
      },
      "source": [
        "### Question 5 (5 points):\n",
        "\n",
        "**a)** Why is it crucial to prevent drastic changes in the Large Language Model's policy during the PPO optimization process?\n",
        "\n",
        "**b)** Explain how PPO addresses the risk of overoptimization or instability in the context of aligning LLMs with human preferences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIhAD8BM1CbA"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scEGGwT91bG5"
      },
      "source": [
        "### Question 6 (5 points):\n",
        "\n",
        "Consider the following simplified form of PPO's objective function used in RLHF:\n",
        "\n",
        "$$\n",
        "\\text{objective}(\\phi) = \\mathbb{E}_{(x,y) \\sim D_{\\pi_{\\phi}^{\\text{RL}}}} \\left[ r_{\\theta}(x, y) - \\beta \\log \\left( \\frac{\\pi_{\\phi}^{\\text{RL}}(y \\mid x)}{\\pi^{\\text{SFT}}(y \\mid x)} \\right) \\right] + \\gamma \\mathbb{E}_{x \\sim D_{\\text{pretrain}}} \\left[ \\log(\\pi_{\\phi}^{\\text{RL}}(x)) \\right]\n",
        "$$\n",
        "\n",
        "**a)** Why does the reward term, $r_{\\theta}(x, y)$ , appear in this objective function even though we are differentiating with respect to the policy parameters, $\\phi$?\n",
        "\n",
        "**b)** What is the role of this term in driving the policy improvement?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzeuZttF2vfz"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_CkMu18uleE"
      },
      "source": [
        "---\n",
        "**Learn More:**\n",
        "<br>[Huggingface Deep Reinforcement Learning Course](https://huggingface.co/learn/deep-rl-course/en/unit0/introduction)\n",
        "<br>[Research Papers for Reinforcement Learning with Human Feedback ](https://github.com/opendilab/awesome-RLHF)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZR7dUWP2cdg"
      },
      "source": [
        "## DPO (25 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbmh23R02d4V"
      },
      "source": [
        "### Question 7 (5 points):\n",
        "<div align=\"center\"><img width=\"80%\" alt=\"image\" src=\"https://miro.medium.com/v2/resize:fit:1400/1*GZnOKpza5yE616uN4OlaVg.jpeg\"></div>\n",
        "\n",
        "**a)** How does Direct Preference Optimization (DPO) differ from RLHF in aligning LLMs? Explain the DPO loss function below and its key terms:\n",
        "\n",
        "$$\n",
        "\\text{L}_{\\text{DPO}}(\\pi_\\theta; \\pi_{\\text{ref}}) = -\\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w | x)}{\\pi_{\\text{ref}}(y_w | x)} - \\beta \\log \\frac{\\pi_\\theta(y_l | x)}{\\pi_{\\text{ref}}(y_l | x)} \\right) \\right]\n",
        "$$\n",
        "\n",
        "**b)** What is the role of the $ \\pi_{\\text{ref}} $ in the DPO loss function, and why is it necessary for stable training?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE0zRF_V26Ag"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh5So3euzEL2"
      },
      "source": [
        "### Load Model & Tokenizer (2.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n6iCFe-2dbx"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67ax_PdzcLw"
      },
      "source": [
        "### Preparing Data (2.5 points)\n",
        "- Load dataset for training.\n",
        "- Convert data into the expected format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ03qXqky_pJ"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(CONFIG.dataset_name, split='train')\n",
        "\n",
        "def filter_responses(row, similarity_threshold=0.6, word_limit=1000):\n",
        "    chosen_text = row['chosen'][-1]['content'] if isinstance(row['chosen'], list) else row['chosen']\n",
        "    rejected_text = row['rejected'][-1]['content'] if isinstance(row['rejected'], list) else row['rejected']\n",
        "    \n",
        "    # Compute similarity score\n",
        "    similarity = ratio(chosen_text, rejected_text)\n",
        "\n",
        "    # Count words in each response\n",
        "    chosen_word_count = len(chosen_text.split())\n",
        "    rejected_word_count = len(rejected_text.split())\n",
        "\n",
        "    # Apply filtering conditions\n",
        "    if similarity >= similarity_threshold:  # Remove if too similar\n",
        "        return False\n",
        "    if chosen_word_count >= word_limit or rejected_word_count >= word_limit:  # Remove if too long\n",
        "        return False\n",
        "    \n",
        "    return True\n",
        "\n",
        "# Apply filtering\n",
        "dataset = dataset.filter(filter_responses)\n",
        "\n",
        "# Select a subset\n",
        "dataset = dataset.shuffle(seed=CONFIG.seed).select(range(CONFIG.train_data_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_dpo_dataset(example):\n",
        "\n",
        "    # WRITE YOUR CODE HERE\n",
        "\n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "        \"chosen\": chosen_response,\n",
        "        \"rejected\": rejected_response\n",
        "    }\n",
        "\n",
        "# Process the dataset\n",
        "dataset = dataset.map(\n",
        "    format_dpo_dataset,\n",
        "    num_proc=12,\n",
        "    remove_columns=[\"source\", \"question\", \"chosen\", \"rejected\"],\n",
        "    desc=\"Formatting dataset for DPO training\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFRcnvyLz8pc"
      },
      "source": [
        "### Applying LoRA Adapters (2.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI6bNnlRzhxx"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVT8N9891Ubw"
      },
      "source": [
        "### Train the Model (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqpZTuCny_f0"
      },
      "outputs": [],
      "source": [
        "# One must patch the DPO Trainer first!\n",
        "from unsloth import PatchDPOTrainer\n",
        "PatchDPOTrainer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gay0MUe_1YnU"
      },
      "outputs": [],
      "source": [
        "from trl import DPOTrainer, DPOConfig\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v1h1ajk10Q3"
      },
      "outputs": [],
      "source": [
        "dpo_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save the Model (2.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JST2NuBT1wLc"
      },
      "source": [
        "### Inference (2.5 points)\n",
        "- Enable faster inference with Unsloth.\n",
        "- Generate output for two randomly selected samples from the `orpo-dpo-mix-40k` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gp-_ME661Yix"
      },
      "outputs": [],
      "source": [
        "sample_prompts = []\n",
        "dpo_responses = []\n",
        "\n",
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23IyRIAW58rW"
      },
      "source": [
        "### Evaluate with Reward Model (2.5 points)\n",
        "\n",
        "- Estimate the rewards of generated responses.\n",
        "\n",
        "    **Note:** Consider memory management in this section. If you encounter an **Out of Memory** issue, you should save the responses after making inferences from the model, free up GPU memory, and then load the Reward Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaCbVjxlyRLm"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeYK6BAl4Z0Z"
      },
      "source": [
        "## ORPO (20 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://arxiv.org/html/2403.07691v1/x2.png\" style=\"background-color:white; padding:10px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN8X5CH04k5Z"
      },
      "source": [
        "### Question 8 (5 points):\n",
        "\n",
        "Traditional preference alignment methods, such as Reinforcement Learning with Human Feedback (RLHF) and Direct Preference Optimization (DPO), often rely on a separate reference model to guide the optimization process. [ORPO](https://arxiv.org/abs/2403.07691), however, eliminates this dependency.\n",
        "\n",
        "**a.** Explain why removing the reference model simplifies preference optimization in language models.\n",
        "\n",
        "**b.** Discuss the potential advantages and disadvantages of this approach compared to RLHF and DPO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhwpNIbspeDA"
      },
      "source": [
        "### Train the model (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Follow the steps as in the DPO section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Set up ORPOTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYj2-3WNY2MY"
      },
      "outputs": [],
      "source": [
        "from trl import ORPOConfig, ORPOTrainer\n",
        "\n",
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfUxFREMYmq8"
      },
      "outputs": [],
      "source": [
        "orpo_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHqExhtpp-BE"
      },
      "source": [
        "### Inference (2.5 points)\n",
        "- Make an inference on two randomly selected samples (similar to the DPO section)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig6ASkWfp6GU"
      },
      "outputs": [],
      "source": [
        "sample_prompts = []\n",
        "orpo_responses = []\n",
        "\n",
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate with Reward Model (5 points)\n",
        "\n",
        "- Estimate the rewards of generated responses.\n",
        "- Compare DPO and ORPO results.\n",
        "\n",
        "    **Note:** Consider memory management in this section. If you encounter an **Out of Memory** issue, you should save the responses after making inferences from the model, free up GPU memory, and then load the Reward Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgHZNB0dqrJ6"
      },
      "source": [
        "### Question 9 (2.5 points):\n",
        "\n",
        "Compare DPO and ORPO in terms of execution time and VRAM used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCPME8yP9Sh5"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Optional Section** (10 points):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Evaluating the Impact of Alignment on ICL**\n",
        "\n",
        "In this section, you will re-evaluate the **in-context learning (ICL) performance** after aligning the model with **DPO** and **ORPO**. The goal is to analyze how alignment affects the modelâ€™s ability to follow different prompting strategies.\n",
        "\n",
        "1. **Use the same evaluation setup** from the [Prompt Engineering](#prompt-engineering) section.\n",
        "2. **Re-run the model** on the same [GSM8K](#gsm8k_benchmark) tasks.\n",
        "3. **Document your observations** in a table:\n",
        "\n",
        "| Model Version  | Accuracy (%) | Common Errors |\n",
        "|---------------|------------|--------------|\n",
        "| Baseline       | XX%        | \\<list errors> |\n",
        "| Post-DPO      | XX%        | \\<list errors> |\n",
        "| Post-ORPO      | XX%        | \\<list errors> |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Discussion:**\n",
        "- Does preference alignment improve or degrade raw performance?\n",
        "- Does the model respond differently to variations in prompts?\n",
        "- How does alignment impact the model's **reasoning consistency** in prompts like CoT?\n",
        "\n",
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDsXcuYW99qc"
      },
      "source": [
        "# AI Disclosure\n",
        "\n",
        "*   Did you use any AI assistance to complete this homework? If so, please also specify what AI you used.\n",
        "    * *Chat GPT*\n",
        "\n",
        "\n",
        "---\n",
        "*(only complete the below questions if you answered yes above)*\n",
        "\n",
        "*   If you used a large language model to assist you, please paste prompts that you used below. Add a separate bullet for each prompt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
