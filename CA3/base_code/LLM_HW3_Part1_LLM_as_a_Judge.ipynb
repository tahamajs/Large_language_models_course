{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CA 3 - Part1, LLMs Spring 2025**\n",
        "\n",
        "- **Name:**\n",
        "- **Student ID:**\n",
        "\n",
        "---\n",
        "#### Your submission should be named using the following format: `CA3 - Part1_LASTNAME_STUDENTID.ipynb`.\n",
        "\n",
        "---\n",
        "\n",
        "##### *How to do this problem set:*\n",
        "\n",
        "- Some questions require writing Python code and computing results, and the rest of them have written answers. For coding problems, you will have to fill out all code blocks that say `YOUR CODE HERE`.\n",
        "\n",
        "- For text-based answers, you should replace the text that says ```Your Answer Here``` with your actual answer.\n",
        "\n",
        "- There is no penalty for using AI assistance on this homework as long as you fully disclose it in the final cell of this notebook (this includes storing any prompts that you feed to large language models). That said, anyone caught using AI assistance without proper disclosure will receive a zero on the assignment (we have several automatic tools to detect such cases). We're literally allowing you to use it with no limitations, so there is no reason to lie!\n",
        "\n",
        "---\n",
        "\n",
        "##### *Academic honesty*\n",
        "\n",
        "- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your notebook. If you turn in correct answers on your notebook without code that actually generates those answers, we will consider this a serious case of cheating.\n",
        "\n",
        "- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n",
        "\n",
        "---\n",
        "\n",
        "If you have any further questions or concerns, contact the TAs via email:"
      ],
      "metadata": {
        "id": "_ze-S8UKMQx-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X4-pPZJ2DwU"
      },
      "source": [
        "# Import libraries and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgivWMATc3ZZ"
      },
      "outputs": [],
      "source": [
        "# Write Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§©Part 1: Judgement Strategies in LLM as a Judge"
      ],
      "metadata": {
        "id": "5PjORxmLhzWZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLJ2f7-91Lrk"
      },
      "source": [
        "## 1.1 Load Dataset\n",
        "\n",
        "In this assignment, you will explore a dataset commonly used for evaluating feedback and alignment in Large Language Models (LLMs). The goal is to help you become familiar with how such datasets are structured and how to extract meaningful information from them.\n",
        "\n",
        " use the ðŸ¤— datasets library to download the following dataset:\n",
        "\n",
        "> `prometheus-eval/Feedback-Bench`\n",
        "\n",
        "> Link: https://huggingface.co/datasets/prometheus-eval/Feedback-Bench\n",
        "\n",
        "> paper: https://arxiv.org/abs/2310.08491\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJUUmYSh13Ni"
      },
      "outputs": [],
      "source": [
        "# Write Your Code Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtBrFXs90umT"
      },
      "outputs": [],
      "source": [
        "# Write Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Summary and Statistical Analysis of Dataset (3 points)\n",
        "In this section, your task is to explore and analyze the dataset both quantitatively and qualitatively.\n",
        "\n",
        "* Describe what the column represents.\n",
        "\n",
        "* Identify columns with integer or numerical values.\n",
        "\n",
        "* Plot the distribution of these columns using histograms or other appropriate visualizations."
      ],
      "metadata": {
        "id": "lNlsby2ANShY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VemOt-wk1xQ_"
      },
      "outputs": [],
      "source": [
        "# Write Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ],
      "metadata": {
        "id": "Y8-7rT39Ml7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Load Phi-3-3.8B model\n",
        "\n",
        "Use the Hugging Face transformers library to load the model and tokenizer:\n",
        "\n",
        "Model: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\n"
      ],
      "metadata": {
        "id": "h9fDj-EsjCGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "BoyLrVsvjBXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Phi Judgemnt Performance Evaluation (23 points)\n",
        "\n",
        "In this part of the assignment, you will assess the ability of the Phi-3-mini model to generate evaluative judgments based on structured prompts derived from the dataset. Follow the steps below to carry out the inference process and evaluate the modelâ€™s performance:\n",
        "\n",
        "**1. Prompt Construction:**\n",
        "\n",
        "\n",
        "Use relevant columns from the dataset (e.g., orig_instruction,orig_criteria, etc.) to construct informative prompts that the model can respond to meaningfully.\n",
        "\n",
        "\n",
        "**2. Model Inference:**\n",
        "\n",
        "Select a random sample of 50 entries from the dataset. For each entry, feed the constructed prompt into the Phi model and generate a corresponding judgment and score.\n",
        "\n",
        "*Don't forget applying chat template ðŸ˜Š*\n",
        "\n",
        "**3. Output Parsing:**\n",
        "\n",
        "After generating model outputs, create a method to extract the predicted score  from the modelâ€™s response.\n",
        "\n",
        "\n",
        "**4. Metric Selection and Performance Analysis:**\n",
        "\n",
        "Compare the predicted scores obtained from the model with the original human-annotated scores available in the `orig_score` column of the dataset. This step will help you measure how well the modelâ€™s outputs align with refrence judge."
      ],
      "metadata": {
        "id": "ftFFnhmKq9Uy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.1 Prompt Construction (2 points)"
      ],
      "metadata": {
        "id": "L2FjGnKoVzcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "vSfK0miRWF9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.2 Model Inference (5 points)"
      ],
      "metadata": {
        "id": "VarMRNQZWG5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "uysvNUNrqbd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.3 Extract Score (Output Parsing) (5 points)"
      ],
      "metadata": {
        "id": "PKPkfJHqyzQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "3z3INKCWyydu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.4 Metric Selection and Performance Analysis (11 points)\n",
        "\n",
        "Respond to the following questions to deepen your understanding of evaluation strategies in LLM-based scoring tasks:\n",
        "\n",
        "\n",
        "What is the most appropriate evaluation metric for comparing the modelâ€™s predicted scores with the reference value (`orig_score`)? Consider the type of scores (e.g., continuous, ordinal, or categorical) when making your choice. (3 points)\n",
        "\n",
        "Calculate the chosen evaluation metric (any suitable metric) to quantify the relationship between the model's predicted score and `orig_score` (6 points).\n",
        "\n",
        "Is accuracy a suitable metric in this context? Why or why not? (2 points)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0GIbqfSO4r4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ],
      "metadata": {
        "id": "M92jh31lNd41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "yKgnROsi56-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ],
      "metadata": {
        "id": "e55bgsxHNed8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Alternative Evaluation Strategies (15 points)\n",
        "\n",
        "In addition to the default scoring approach, you are encouraged to explore alternative judgment strategies to evaluate the modelâ€™s performance on the judgment task.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Examples of Alternative Approaches\n",
        "\n",
        "#### Quantetive Prompt Design\n",
        "- Reformulate the prompts to request a **score on a different scale**, such as from **1 to 100** instead of 1 to 5.\n",
        "- After model inference, **normalize** or **map** the predicted score back to the **1â€“5 range** for comparison (e.g., using simple scaling or binning).\n",
        "\n",
        "#### Qualitative Scoring (Likert-style)\n",
        "- Design prompts to elicit **descriptive judgments**, such as:  \n",
        "  `\"Poor\"`, `\"Fair\"`, `\"Good\"`, `\"Very Good\"`, `\"Excellent\"`\n",
        "- Then **map these qualitative outputs** to **numerical values** (e.g., 1 to 5) to enable metric-based evaluation.\n",
        "\n"
      ],
      "metadata": {
        "id": "aFnLvKIR8mmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "KI7bUlm-UO0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "P7qDS0cSqRgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "5OQgN6N0qRUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "v0vc3MJFqUMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ðŸ§© Part 2: Creating Preference Data Using LLM as Judge\n",
        "\n",
        "In this part, you will explore how to use large language models (LLMs) to generate **preference data** for optimization tasks.\n",
        "\n",
        "We will compare two models:\n",
        "\n",
        "- `Qwen/Qwen1.5-1.8B-Chat`\n",
        "- `stabilityai/stablelm-2-zephyr-1_6b`\n",
        "\n",
        "The goal is to evaluate how well these models can **distinguish preferred answers (\"chosen\") from less favorable ones (\"rejected\")** in a human-like manner.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "eE91AL2PqZfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Download the Models and Dataset\n",
        "\n",
        "- Load the following two models from Hugging Face:\n",
        "  - `Qwen/Qwen1.5-1.8B-Chat`\n",
        "  - `stabilityai/stablelm-2-zephyr-1_6b`\n",
        "\n",
        "- Download the dataset:  \n",
        "  [`HumanLLMs/Human-Like-DPO-Dataset`](https://huggingface.co/datasets/HumanLLMs/Human-Like-DPO-Dataset)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-Trlh0ydZjhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "InUGnZdfJL8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "1S_traP8I-Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "_4S11PbRZ0Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Dataset Exploration (1 point)\n",
        "\n",
        "\n",
        "- Analyze the `HumanLLMs/Human-Like-DPO-Dataset`.\n",
        "  - Describe the dataset structure and columns.\n",
        "\n",
        "- **Optional**: Read the paper for additional context and insights:  \n",
        "   [Human-Like DPO (arXiv:2501.05032)](https://arxiv.org/pdf/2501.05032)\n",
        "\n"
      ],
      "metadata": {
        "id": "2kohpa-vOHUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ],
      "metadata": {
        "id": "l0EkXgwiNz3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Judging Setup (3 points)\n",
        "\n",
        "- Create a **prompting framework** that presents both the **chosen** and **rejected** answers to the model and asks it to **select the better one**.\n",
        "\n",
        "\n",
        "Example prompt structure:\n",
        "> \"Here is a prompt and two responses. Please choose the better response based on helpfulness, relevance, and coherence.  \n",
        ">  \n",
        "> Prompt: {prompt}  \n",
        ">  \n",
        "> Response 1: {chosen or rejected}  \n",
        "> Response 2: {rejected or chosen}  \n",
        ">  \n",
        "> Which response is better? Reply with 'Answer 1' or 'Answer 2'.\"\n",
        "---"
      ],
      "metadata": {
        "id": "N4e0fakr9Orq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Write Your Code Here"
      ],
      "metadata": {
        "id": "fykTRxZzcFeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Model Comparison (10 points)\n",
        "\n",
        "- Run inference using both models on a **sample of the dataset** (e.g., 200â€“500 instances from dataset). (2 points)\n",
        "- Compare each model's judgments to the **ground truth** (i.e., whether it preferred the \"chosen\" response). (4 points)\n",
        "- Compute the **accuracy** and plot **confusion matrix** for each model to evaluate performance. (4 points)\n",
        "- Make sure to properly handle cases where the model's output is unclear or the preference cannot be extracted (e.g., skip or categorize as \"unkowned\")."
      ],
      "metadata": {
        "id": "N0lpkq0NPbaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "AyK2-BsiJtnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "mnZM43PMqwD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "qrisL91j9OEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Your Code Here"
      ],
      "metadata": {
        "id": "aKvtiLs3nrrl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "2X4-pPZJ2DwU"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}