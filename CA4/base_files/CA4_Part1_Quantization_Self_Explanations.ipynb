{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_aFoOt_p3i_"
      },
      "source": [
        "## CA 4, LLMs Spring 2025\n",
        "\n",
        "- **Name:**\n",
        "- **Student ID:**\n",
        "\n",
        "---\n",
        "#### Your submission should be named using the following format: `CA4_Part1_LASTNAME_STUDENTID.ipynb`.\n",
        "\n",
        "---\n",
        "TA Email: melika.nobakhtian2000@gmail.com\n",
        "\n",
        "##### *How to do this problem set:*\n",
        "\n",
        "- Some questions require writing Python code and computing results, and the rest of them have written answers. For coding problems, you will have to fill out all code blocks that say `YOUR CODE HERE`.\n",
        "\n",
        "- For text-based answers, you should replace the text that says ```Your Answer Here``` with your actual answer.\n",
        "\n",
        "- There is no penalty for using AI assistance on this homework as long as you fully disclose it in the final cell of this notebook (this includes storing any prompts that you feed to large language models). That said, anyone caught using AI assistance without proper disclosure will receive a zero on the assignment (we have several automatic tools to detect such cases). We're literally allowing you to use it with no limitations, so there is no reason to lie!\n",
        "\n",
        "---\n",
        "\n",
        "##### *Academic honesty*\n",
        "\n",
        "- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your notebook. If you turn in correct answers on your notebook without code that actually generates those answers, we will consider this a serious case of cheating.\n",
        "\n",
        "- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWGLopwZ4z3Q"
      },
      "source": [
        "# Quantization (37 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4p1gDPB5_Yy"
      },
      "source": [
        "Quantization is a technique used to reduce the precision of neural network weights and activations, typically from floating-point to a lower-bit representation, such as 8-bit or 4-bit integers. The primary goal of quantization is to reduce the memory footprint and computational requirements of deep learning models, enabling the loading of larger models that would normally not fit into available memory, and speeding up the inference process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x36dBcBT6Obb"
      },
      "source": [
        "## A simple example (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dFkf7f06Z-x"
      },
      "source": [
        "Let's see what happens when a we quantize a 32-bit floating-point number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8_FGiu8Z4oxi"
      },
      "outputs": [],
      "source": [
        "# Import neccesary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSD-3uGI6imR"
      },
      "source": [
        "Defining two functions which responsible for quantizing and dequantizing the input number:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lWA4KcxD6q5l"
      },
      "outputs": [],
      "source": [
        "def quantize(value, num_bits=4):\n",
        "    quantized_value = np.round(value * (2**(num_bits - 1) - 1))\n",
        "    return int(quantized_value)\n",
        "\n",
        "def dequantize(quantized_value, num_bits=4):\n",
        "    value = quantized_value / (2**(num_bits - 1) - 1)\n",
        "    return float(value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9fg8sQp63QM"
      },
      "source": [
        "Consider the value `0.415`, the quantized values in 4 and 8 bits are:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUcjKVkf69yo"
      },
      "outputs": [],
      "source": [
        "q_4bit = quantize(value=0.415, num_bits=4)\n",
        "q_8bit = quantize(value=0.415, num_bits=8)\n",
        "\n",
        "print(f'4-bit: {q_4bit}')\n",
        "print(f'8-bit: {q_8bit}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kptUMpda7Axu"
      },
      "source": [
        "And if we dequantize it to original full precision values we would have:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydtKlg2i7Rgi"
      },
      "outputs": [],
      "source": [
        "print(f'4-bit: {dequantize(quantized_value=q_4bit, num_bits=4)}')\n",
        "print(f'8-bit: {dequantize(quantized_value=q_8bit, num_bits=8)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg4m2FQg7bR2"
      },
      "source": [
        "8-bit quantization preserves the original precision with very little degradationa and 4-bit quantization does incur more precision loss, but the level of loss can still be tolerated for many applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HldZAJCH7cHR"
      },
      "source": [
        "To understand the precision loss from 4-bit and 8-bit quantization, plot the function $y = x^2$ in the range of $[-1, 1]$, and compare the original values to the values obtained after quantization and dequantization for both 4-bit and 8-bit cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fPJ-RuZlTKs"
      },
      "source": [
        "**# Write your answer here**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dx86yCld7m5z"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfZQZyZrFSP3"
      },
      "source": [
        "## 4-bit quantization and QLoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0sKgxs3FX3H"
      },
      "source": [
        "### Install requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_uIGUPt9ChS"
      },
      "source": [
        "*You may need to restart the session after installation.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wA_wz3YFkaw",
        "outputId": "6951f10e-5436-4c9c-b98b-8d23f2deca91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6zqiitn9sr0"
      },
      "source": [
        "### Comparing Models (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e9PRWj5-V5l"
      },
      "source": [
        "In this part, you should load a model in two modes: standard and 4-bit mode. After loading model in two modes, print each model. What differences you see between these two versions of model? Why we have these differences?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCXWuS_SlXwR"
      },
      "source": [
        "**# Write your answer here**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OVTQkc8_8dv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"facebook/opt-350m\"\n",
        "\n",
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsCUa553AnOf"
      },
      "source": [
        "Try to inference from both of these models with asking them to continue a sentence. Is there any difference between their outputs? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTb9a8dylcF-"
      },
      "source": [
        "**# Write your answer here**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPVATUGPcVrM"
      },
      "outputs": [],
      "source": [
        "text = \"Welcome! This is\"\n",
        "device = \"cuda:0\"\n",
        "\n",
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsorvKliELbU"
      },
      "source": [
        "### Advanced Quantization with BitsAndBytes (6 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp3MW9gsEywL"
      },
      "source": [
        "In this part, again we want to load a quantized version of our desired model (in 4 bit) but this time with `BitsAndBytesConfig` and more advanced settings. Answer the following questions about different parameters of config or explain about the. Then use them to define the suitable config and load model with it.\n",
        "\n",
        "\n",
        "*   Explain about `compute_dtype`, its different modes and the differences among these modes.\n",
        "*   The 4bit integration comes with 2 different quantization types: FP4 and NF4. Explain about them and talk about their differences.\n",
        "* We can use nested quantization with setting `bnb_4bit_use_double_quant=True`. What is this and What can we do with this?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RG6mDLHEvXj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjUcA6DkOPyP"
      },
      "source": [
        "Again try to make inference from this model by completting a sentence. Is there any difference with previous modes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUbh0FOHTNSc"
      },
      "outputs": [],
      "source": [
        "text = \"Welcome! This is\"\n",
        "device = \"cuda:0\"\n",
        "\n",
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAKMQZtInvxQ"
      },
      "source": [
        "## Fine-Tune Gemma using QloRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjG8kSbPrAr8"
      },
      "source": [
        "In this part, you will find out how to fine-tune Gemma on a custom text-to-sql dataset using Hugging Face Transformers and TRL. You will use and learn about:\n",
        "\n",
        "* Quantized Low-Rank Adaptation (QLoRA)\n",
        "* Setup development environment\n",
        "* Create and prepare the fine-tuning dataset\n",
        "* Fine-tune Gemma using TRL and the SFTTrainer\n",
        "* Test Model Inference and generate SQL queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFJI6YXBhRSN"
      },
      "source": [
        "### Setup environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW22burjh-RW"
      },
      "source": [
        "*You may need to restart the session after installation.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_744JlchYV-"
      },
      "outputs": [],
      "source": [
        "!pip install \"torch>=2.4.0\" tensorboard\n",
        "\n",
        "!pip install \"transformers>=4.51.3\"\n",
        "\n",
        "\n",
        "!pip install  --upgrade \\\n",
        "  \"datasets==3.3.2\" \\\n",
        "  \"accelerate==1.4.0\" \\\n",
        "  \"evaluate==0.4.3\" \\\n",
        "  \"bitsandbytes==0.45.3\" \\\n",
        "  \"trl==0.15.2\" \\\n",
        "  \"peft==0.14.0\" \\\n",
        "  protobuf \\\n",
        "  sentencepiece\n",
        "\n",
        "!pip uninstall protobuf python3-protobuf\n",
        "!pip install --upgrade pip\n",
        "!pip install --upgrade protobuf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir7O4KSeidPk"
      },
      "source": [
        "### Hugging Face Login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKr37V6hioOr"
      },
      "source": [
        "For some language models, you need to agree to share your contact information to access the model. `gemma-3-1b-pt` is one of them. The steps you should take are as follows:\n",
        "\n",
        "1.   Create a Gugging Face account if you don't have one.\n",
        "2.   From Settings > Access Tokens, generate a new token. Your access token should have both read and write permissions.\n",
        "3.   From [this link](https://huggingface.co/google/gemma-3-1b-pt) agree to access the repository.\n",
        "\n",
        "Now, run the code below to login to your account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vzsky5fojQwz"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "access_token = \"\"\n",
        "login(token = access_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzkEGc5SkCjY"
      },
      "source": [
        "### Create and prepare the fine-tuning dataset (6 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05DiPid_1hLn"
      },
      "source": [
        "\n",
        "\n",
        "Our purpose ia to fine-tune a natural language to SQL model for seamless integration into a data analysis tool. Now, we need a dataset to fine-tune.\n",
        "\n",
        "Here we use this dataset [philschmid/gretel-synthetic-text-to-sql](https://huggingface.co/datasets/philschmid/gretel-synthetic-text-to-sql), a high quality synthetic Text-to-SQL dataset including natural language instructions, schema definitions, reasoning and the corresponding SQL query.\n",
        "\n",
        "Hugging Face TRL supports automatic templating of conversation dataset formats. This means you only need to convert your dataset into the right json objects, and trl takes care of templating and putting it into the right format.\n",
        "\n",
        "This dataset contains over 100k samples. But now you should only use 5000 samples and 1000 samples from that will be used for test dataset.\n",
        "\n",
        "You should now use the Hugging Face Datasets library to load the dataset and create a prompt template to combine the natural language instruction, schema definition and add a system message for your assistant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFX5yQlk46wq"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# System message for assistant\n",
        "system_message = \"\"\" \"\"\"\n",
        "\n",
        "# User prompt that combines the 'user query' and the 'schema' (context) from dataset\n",
        "user_prompt = \"\"\" \"\"\"\n",
        "\n",
        "# WRITE YOUR CODE HERE\n",
        "# Complete this function\n",
        "def create_conversation(sample):\n",
        "  return {\n",
        "    \"messages\": [\n",
        "      {\"role\": \"system\", \"content\": ... },\n",
        "      {\"role\": \"user\", \"content\": ... },\n",
        "      {\"role\": \"assistant\", \"content\": ... }\n",
        "    ]\n",
        "  }\n",
        "\n",
        "# Load dataset from the hub\n",
        "dataset = ...\n",
        "\n",
        "# Mapping dataset and split it to train and test\n",
        "# WRITE YOUR CODE HERE\n",
        "...\n",
        "\n",
        "# Print formatted user prompt\n",
        "# WRITE YOUR CODE HERE\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJZyteem8K-a"
      },
      "source": [
        "### Fine-tune Gemma using TRL and the SFTTrainer (12 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjRLbUQW8Gbq"
      },
      "source": [
        "\n",
        "You are now ready to fine-tune your model. Hugging Face TRL SFTTrainer makes it straightforward to supervise fine-tune open LLMs. The SFTTrainer is a subclass of the Trainer from the transformers library and supports all the same features, including logging, evaluation, and checkpointing, but adds additional quality of life features, including:\n",
        "\n",
        "* Dataset formatting, including conversational and instruction formats\n",
        "* Training on completions only, ignoring prompts\n",
        "* Packing datasets for more efficient training\n",
        "* Parameter-efficient fine-tuning (PEFT) support including QloRA\n",
        "* Preparing the model and tokenizer for conversational fine-tuning (such as adding special tokens)\n",
        "\n",
        "Complete the following code that loads the Gemma model and tokenizer from Hugging Face and initializes the quantization configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0D10jRGX_fVw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForImageTextToText, BitsAndBytesConfig\n",
        "\n",
        "# Hugging Face model id\n",
        "model_id = \"google/gemma-3-1b-pt\"\n",
        "\n",
        "\n",
        "# Define model init arguments\n",
        "# WRITE YOUR CODE HERE\n",
        "model_kwargs = dict(\n",
        "    attn_implementation= ...,\n",
        "    torch_dtype=...\n",
        "    device_map=...\n",
        ")\n",
        "\n",
        "# WRITE YOUR CODE HERE\n",
        "# BitsAndBytesConfig: Enables 4-bit quantization to reduce model size/memory usage\n",
        "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
        "    .....\n",
        ")\n",
        "\n",
        "# WRITE YOUR CODE HERE\n",
        "# Load model and tokenizer\n",
        "model = ...\n",
        "tokenizer = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LddoWlzIB9q7"
      },
      "source": [
        " In this part, You only need to create a LoraConfig and to provide it to the SFTtrainer in the next parts Try to create a LoRA configuration with rank and alpha parameter both equal to 16."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCLkFxSdFPb6"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "# WRITE YOUR CODE HERE\n",
        "peft_config = LoraConfig(\n",
        "    .....\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1n1k0WTGWiv"
      },
      "source": [
        "Before you can start your training, you need to define the hyperparameter you want to use in a SFTConfig instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCgfLzZsGZ5W"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig\n",
        "\n",
        "# WRITE YOUR CODE HERE\n",
        "args = SFTConfig(\n",
        "    ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl1dieVqHcFl"
      },
      "source": [
        "You now have every building block you need to create your SFTTrainer to start the training of your model. Start training model and then save it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwYCYDQ2H2Ki"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "# WRITE YOUR CODE HERE\n",
        "trainer = SFTTrainer(\n",
        "    ..............\n",
        ")\n",
        "\n",
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrnqTsLZKNyN"
      },
      "source": [
        "Before you can test your model, make sure to free the memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBlj3LtVKPBn"
      },
      "outputs": [],
      "source": [
        "# free the memory again\n",
        "del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Dr0pW8fKsbF"
      },
      "source": [
        "### Test Model Inference and generate SQL queries (7 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwphl2WLLBDt"
      },
      "source": [
        "After the training is done, you should  evaluate and test your model. You should load some samples from the test ataset and evaluate the model on those samples. You do not need to evaluate them based on specific metric. Just try to see different outputs and evaluate them manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JIhTHGuLAVR"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q63J8IdkUGnt"
      },
      "source": [
        "# Self-Explanations (13 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtPyu5m-WuQx"
      },
      "source": [
        "In this section, we will explore the fascinating world of LLM self-explanations,\n",
        "focusing on two main approaches:\n",
        "* **Explanation-to-Prediction (E-P)**\n",
        "* **Prediction-to-Explanation (P-E)**\n",
        "\n",
        "You'll implement both techniques and analyze their effectiveness in sentiment analysis tasks.\n",
        "\n",
        "Paper: https://arxiv.org/pdf/2310.11207\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9HH0ipWYXje"
      },
      "source": [
        "## Setup and Imports (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8mycHSvY1Bd"
      },
      "source": [
        "In this part, you should setup OpenAI client. You sould create an account in https://openrouter.ai/ and get a key to use it in the next parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "37_5EH-9UYdU"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# WRITE YOUR CODE HERE\n",
        "open_router_key = \"\"\n",
        "\n",
        "client = ...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euwgtxxHagpN"
      },
      "source": [
        "## Conceptual Understanding (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKTPf4c0ajfr"
      },
      "source": [
        "\n",
        "\n",
        "**Q1**: What are the two main approaches to LLM self-explanations discussed in the paper?\n",
        "Briefly describe each approach.\n",
        "\n",
        "\n",
        "**Q2**: According to the research, what doubt is cast on LLM explanations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4DboMGjb5yB"
      },
      "source": [
        "**# Write your answer here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ6uaZCwckNS"
      },
      "source": [
        " ## Explanation-to-Prediction (E-P) (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA5N-0h6eit4"
      },
      "source": [
        "In this part, we will implement E-P for self-explanation for some movie reviews based on paper. Based on paper, you should:\n",
        "- Complete the E-P prompt template\n",
        "- Implement the function to call model API\n",
        "- Parse and analyze the response\n",
        "\n",
        "**To select model, you can use different models from openrouter that have free API credit.**\n",
        "https://openrouter.ai/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqcCemOhma6h"
      },
      "outputs": [],
      "source": [
        "# Sample movie reviews for testing\n",
        "sample_reviews = [\n",
        "    \"Offers that rare combination of entertainment and education that makes for great family viewing.\",\n",
        "    \"A film that takes you inside the rhythms of its subject with remarkable intimacy.\",\n",
        "    \"The movie was absolutely terrible, with poor acting and a confusing plot.\",\n",
        "    \"An outstanding masterpiece that will be remembered for years to come.\"\n",
        "]\n",
        "\n",
        "\n",
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtt3KJDM4F6p"
      },
      "source": [
        "## Prediction-to-Explanation (P-E) (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CemajSIW4Lmx"
      },
      "source": [
        "In this part, we will implement P-E for self-explanation for some movie reviews based on paper. Based on paper, you should:\n",
        "- Complete the E-P prompt template\n",
        "- Implement the function to call model API\n",
        "- Parse and analyze the response\n",
        "\n",
        "**To select model, you can use different models from openrouter that have free API credit.**\n",
        "https://openrouter.ai/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCi9CmGk4RUb"
      },
      "outputs": [],
      "source": [
        "# Sample movie reviews for testing\n",
        "sample_reviews = [\n",
        "    \"Offers that rare combination of entertainment and education that makes for great family viewing.\",\n",
        "    \"A film that takes you inside the rhythms of its subject with remarkable intimacy.\",\n",
        "    \"The movie was absolutely terrible, with poor acting and a confusing plot.\",\n",
        "    \"An outstanding masterpiece that will be remembered for years to come.\"\n",
        "]\n",
        "\n",
        "\n",
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaLxWsIC6Ier"
      },
      "source": [
        "# Comparative Analysis (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KubpQ0IR6XWn"
      },
      "source": [
        "Compare Results from both E-P and P-E and talk about the effectiveness of each approach:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WbkE42v6ouO"
      },
      "source": [
        "**# Write your answer here**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
